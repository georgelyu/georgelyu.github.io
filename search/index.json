[{"content":"本篇文章主要讲如何通过 mma ptx 语句来调用 NVIDIA GPU 的 Tensor Core。\n主要概念 Tensor Core 执行的是 M-by-N-by-K 的矩阵操作 D = op(A, B) + C，这里使用的是 BLAS 的定义，即 A 矩阵的维度是 M x K，B 矩阵的维度是 K x N，C 和 D 矩阵的维度是 M x N。我们把 [M, N, K] 的组合称为 shape。\nTensor Core 在执行这样的矩阵操作时是 warp-synchronous 的模式，即 warp 中所有线程同时参与运算。而对于拥有 32 个线程的 warp 来说，很明显有一些 shape 对于 warp 来说会比较好处理，如下面讲的 fundamental shape。\nFundamental Shape Tensor Core 操作中的 fundamental shape 是 8-by-8-by-128b，见下图。\n在图中，蓝色对应矩阵 A，黄色对应矩阵 B，绿色则对应矩阵 C 或 D。因为一个 warp 有 32 个线程，我们可以把这 32 个线程排布成一个 8 x 4 和 4 x 8 的矩阵，分别对应蓝色和黄色部分。如果一个线程持有一个 32b 的数据（可以以图中的 T0 为视角），则依据图示，我们可以看到 K 维度是 128b，所以我们称这个操作是 8-by-8-by-128b 的。我们注意到 T0 拥有两个 accumulator，为了获得 T0 线程的结果，我们需要的是矩阵 A（图中蓝色）的第一行（即 T0-T3 部分），和矩阵 B（图中黄色）的第一、二列（即 T0-T7 部分）。\n这个示例就对应了一个实际的 Tensor Core 操作，即 mma.m8n8k16，见下图。\n这里 mma.m8n8k16 针对的场景是 S8 * S8 + S32，即 A 和 B 的类型是有符号单字节整型，accumulator 是有符号 32b 整型。所以我们看到图中，在矩阵 A 中，T0 拥有的 32 位数据对应了 4 个元素，以 little-endian 形式存储。图右侧显示了实际的 ptx 指令，这里每个线程只需要开一个 32b 寄存器作为输入（A 和 B），但是需要两个 32b 寄存器作为 accumulator，因为每个结果元素是以 32b 进行存储的。\n这里顺便讲一下这里 mma 指令的语法，大体上的格式是\n1 mma.sync.aligned.shape.alayout.blayout.dtype.atype.btype.ctype d, a, b, c; 这里\nmma 是表示 Matrix Multiply-and-Accumulate sync 表示这句指令之前隐含一个 warp-level sync aligned 表示 warp 内所有线程都会执行这同一句指令，如果这一点没有被保证会得到 undefined behavior shape 就是这里讲的 [M, N, K] 组合 alayout、blayout 指的是矩阵 A 和 B 的数据排布（row-major 或 column-major） dtype、atype、btype、ctype 指的就是这些矩阵的数据类型了 当然这些参数之间互相还有一些限制，还有一些额外的参数，详情参考官方文档。\n扩展 M 和 K 维度 在有了 fundamental shape 之后，如果我们想要算更大的 shape 怎么办。很简单的一个想法是，我们直接按照上面的方式再算一次，先从 M 维度开始（见下图）。\n这对应了 F16 * F16 + F32 下面的 mma.m16n8k8 操作。\n那么当然，同理，K 维度也可以得到提升，得到 16-by-8-by-256b 操作。\n这对应了 F16 * F16 + F32 下面的 mma.m16n8k16 操作。\n同时也对应了 S8 * S8 + F32 下面的 mma.m16n8k32 操作。\n数据的加载 我们看到，mma 语句主要负责矩阵的计算部分，而矩阵数据的加载也是重要的一环。我们先展示一个 mma 语句完整的 Hellow World 示例（见下图）。\n而我们分析一下上图中的 kernel 就可以发现这个 kernel 是严重的 bandwidth-bound（分析过程见下图）。\n为了加快数据的加载，我们主要有三点可以做：\n低延迟地加载 global memory 无冲突的 shared memory 存储 无冲突的 shared memory 读取 下面我们每一点分别来讲。\n低延迟地加载 global memory 这里主要的做法是使用 Async Copy（cp.async）。\n这个做法有三点好处：\n数据的移动路径变短：之前数据是从 L2 → L1 → RF → SMEM，而 cp.async 会让数据直接从 L2 → SMEM； 减少寄存器用量：因为 RF 被绕过了，所以寄存器的用量也减少了。而且因为这些 load 语句的延迟很高，这些寄存器其实被长时间占用，不能有效地分给 kernel 的其他部分； 间接影响：异步拷贝现在可以让我们在 kernel 中进行多 stage 操作，这样我们可以在 kernel 计算时去取数据。如下图最下面的部分，我们可以看到展示了一段循环内存，其中 committed stage 是正在从 SMEM 取数据进行计算的部分，而后面有三个阶段在等待取数据，这样最大限度地喂饱计算单元。 无冲突的 shared memory 存储 / 读取 这里我们先介绍一下 ldmatrix 指令。ldmatrix 是一类特殊的，为了加载 tensor core 所需数据而出现的指令。它加载进来的数据排布是完美迎合了 Tensor Core 操作的。我们看到下图的右下角部分。\nldmatrix 工作的原理是，每个线程会有一个指针，指向一行 128b 的数据（见蓝色中黑框框中的 T0-T3 部分），这个线程会读取这 128b 的数据，然后广播给真正需要它的线程们。下图是实际 ptx 指令的示例，我们可以看到这里用了 x4 的指令，即加载了四个蓝色部分的矩阵，所以每个线程加载的数据都没有浪费。\n然后为了避免 bank-conflict，我们需要使用 permuted indices，如下图。\n这个操作需要我们在使用 ldmatrix 进行加载时，把每个线程得到的序号手动错开。\n","date":"2025-03-25T14:23:56+08:00","permalink":"https://georgelyu.github.io/p/tensor_core_ptx/","title":"通过 MMA 使用 NVIDIA GPU 的 Tensor Core"},{"content":"以下内容翻译自 CUDA 官方的 PTX 使用说明，并进行了一些整理。\nASM 命令 我们从 ASM 指令的格式讲起，ASM 指令的格式如下：\n1 asm(\u0026#34;template-string\u0026#34; : \u0026#34;constraint\u0026#34;(output) : \u0026#34;constraint\u0026#34;(input)); 一条简单的 ASM 语句如下所示：\n1 asm(\u0026#34;add.s32 %0, %1, %2;\u0026#34; : \u0026#34;=r\u0026#34;(i) : \u0026#34;r\u0026#34;(j), \u0026#34;r\u0026#34;(k)); 从这个格式和示例，我们可以注意到以下两点：\n在 asm() 括号内的内容分为三部分，分别为模板字符串、输出和输入，这三部分以冒号分隔，但在输入输出中的不同变量以逗号分隔。 输出和输入前都有一个 “约束”。 接下来我们依次讨论一下这两点的细节，先从模板字符串开始。\n模板字符串 在模板字符串中主要需要注意的就是 “%n” 了。这里的 “n” 对应了后面操作符的序号，即 “%0” 对应第一个操作符，“%1” 对应第二个操作符，以此类推。所以顺序可以任意指定，如之前的示例\n1 asm(\u0026#34;add.s32 %0, %1, %2;\u0026#34; : \u0026#34;=r\u0026#34;(i) : \u0026#34;r\u0026#34;(j), \u0026#34;r\u0026#34;(k)); 在概念上对应\n1 add.s32 i, j, k; 而\n1 asm(\u0026#34;add.s32 %0, %2, %1;\u0026#34; : \u0026#34;=r\u0026#34;(i) : \u0026#34;r\u0026#34;(k), \u0026#34;r\u0026#34;(j)); 在概念上和上面的语句是相同的。同时同一个操作符也可以重复出现，如\n1 asm(\u0026#34;add.s32 %0, %1, %1;\u0026#34; : \u0026#34;=r\u0026#34;(i) : \u0026#34;r\u0026#34;(k)); 这在概念上等同于\n1 add.s32 i, k, k; 省略 当没有输入操作符时，后面的冒号部分可以省略掉，如：\n1 asm(\u0026#34;mov.s32 %0, 2;\u0026#34; : \u0026#34;=r\u0026#34;(i)); 而当没有输出操作符时，输出部分的内容可以空置，如：\n1 asm(\u0026#34;mov.s32 r1, %0;\u0026#34; :: \u0026#34;r\u0026#34;(i)); 转义 当需要在 PTX 指令中使用 “%” 时，需要用 “%%” 进行转义：\n1 asm(\u0026#34;mov.u32 %0, %%clock;\u0026#34; : \u0026#34;=r\u0026#34;(x)); 多条语句 在一个 asm() 语句中可以放置多个语句。为了在 PTX 中间文件中生成可读的代码，最好在每一个指令后面用 “\\n\\t” 结尾，如\n1 2 3 4 5 6 7 8 9 __device__ int cube (int x) { int y; asm(\u0026#34;.reg .u32 t1;\\n\\t\u0026#34; // temp reg t1 \u0026#34; mul.lo.u32 t1, %1, %1;\\n\\t\u0026#34; // t1 = x * x \u0026#34; mul.lo.u32 %0, t1, %1;\u0026#34; // y = t1 * x : \u0026#34;=r\u0026#34;(y) : \u0026#34;r\u0026#34; (x)); return y; } 与\n1 2 3 4 5 6 7 8 9 10 11 __device__ int cond (int x) { int y = 0; asm(\u0026#34;{\\n\\t\u0026#34; \u0026#34; .reg .pred %p;\\n\\t\u0026#34; \u0026#34; setp.eq.s32 %p, %1, 34;\\n\\t\u0026#34; // x == 34? \u0026#34; @%p mov.s32 %0, 1;\\n\\t\u0026#34; // set y to 1 if true \u0026#34;}\u0026#34; // conceptually y = (x==34)?1:y : \u0026#34;+r\u0026#34;(y) : \u0026#34;r\u0026#34; (x)); return y; } 约束 寄存器约束 在上面的示例中看到的约束都有字母 “r”，这里的 “r” 指的是 32 位整数寄存器。关于寄存器的约束列表如下：\n1 2 3 4 5 6 \u0026#34;h\u0026#34; = .u16 reg \u0026#34;r\u0026#34; = .u32 reg \u0026#34;l\u0026#34; = .u64 reg \u0026#34;q\u0026#34; = .u128 reg \u0026#34;f\u0026#34; = .f32 reg \u0026#34;d\u0026#34; = .f64 reg 注意 “q” 约束只能在支持 __int128 的机器上使用。\n立即整数约束 同时还有 “n” 约束，表示已知的立即整数（immediate integer operands），如\n1 asm(\u0026#34;add.u32 %0, %0, %1;\u0026#34; : \u0026#34;=r\u0026#34;(x) : \u0026#34;n\u0026#34;(42)); 这在概念上等于\n1 add.u32 r1, r1, 42; 常字符数组约束 约束 “C” 用来表示常字符数组（array of const char），这个字符数组的内容必须是编译时已知的。这个的主要目的是在编译时改变 PTX 命令的 “modes”，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 constexpr int mode_rz = 0; constexpr int mode_rn = 1; template \u0026lt;int mode\u0026gt; struct helper; template\u0026lt;\u0026gt; struct helper\u0026lt;mode_rz\u0026gt; { static constexpr const char mode[] = \u0026#34;.rz\u0026#34;; }; template\u0026lt;\u0026gt; struct helper\u0026lt;mode_rn\u0026gt; { static constexpr const char mode[] = \u0026#34;.rn\u0026#34;; }; template \u0026lt;int rounding_mode\u0026gt; __device__ float compute_add(float a, float b) { float result; asm (\u0026#34;add.f32%1 %0,%2,%3;\u0026#34; : \u0026#34;=f\u0026#34;(result) : \u0026#34;C\u0026#34;(helper\u0026lt;rounding_mode\u0026gt;::mode), \u0026#34;f\u0026#34;(a), \u0026#34;f\u0026#34;(b)); return result; } __global__ void kern(float *result, float a, float b) { *result++ = compute_add\u0026lt;mode_rn\u0026gt;(a,b); // generates add.f32.rn *result = compute_add\u0026lt;mode_rz\u0026gt;(a,b); // generates add.f32.rz } 我们现在知道 “C” 约束后面跟的应该是一个字符数组地址，这个地址指向的变量 V 必须满足下面的约束：\nV 是 static 存储的； V 的类型是 array of const char； V 是用常量初始化的； 如果 V 是一个 static class 的成员，V 的初始化必须也在这个类中。 并且，如果 V 中有 \u0026lsquo;\\0\u0026rsquo; 或 \u0026lsquo;0\u0026rsquo; 作为结尾，这个结尾会被去除。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 struct S1 { static constexpr char buf1[] = \u0026#34;Jumped\u0026#34;; static constexpr char buf2[] = {\u0026#39;O\u0026#39;, \u0026#39;v\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;r\u0026#39;, 0}; }; template \u0026lt;const char *p1, const char *p2, const char *p3\u0026gt; __device__ void doit() { asm volatile (\u0026#34;%0 %1 %2\u0026#34; : : \u0026#34;C\u0026#34;(p1), \u0026#34;C\u0026#34;(p2), \u0026#34;C\u0026#34;(p3)); } struct S2 { static const char buf[]; }; const char S2::buf[] = \u0026#34;this\u0026#34;; const char buf3[] = \u0026#34;Jumped\u0026#34;; extern const char buf4[]; __global__ void foo() { static const char v1[] = \u0026#34;The\u0026#34;; static constexpr char v2[] = \u0026#34;Quick\u0026#34;; static const char v3[] = { \u0026#39;B\u0026#39; , \u0026#39;r\u0026#39; , \u0026#39;o\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;n\u0026#39;, 0 }; static constexpr char v4[] = { \u0026#39;F\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;x\u0026#39;, 0 }; //OK: generates \u0026#39;The Quick Brown Fox Jumped Over\u0026#39; in PTX asm volatile (\u0026#34;%0 %1 %2 %3 %4 %5\u0026#34; : : \u0026#34;C\u0026#34;(v1) , \u0026#34;C\u0026#34;(v2), \u0026#34;C\u0026#34;(v3), \u0026#34;C\u0026#34;(v4), \u0026#34;C\u0026#34;(S1::buf1), \u0026#34;C\u0026#34;(S1::buf2) ); //OK: generates \u0026#39;Brown Fox Jumped\u0026#39; in PTX doit\u0026lt;v3, v4, buf3\u0026gt;(); //error cases const char n1[] = \u0026#34;hi\u0026#34;; //error: argument to \u0026#34;C\u0026#34; constraint is not a constant expression asm volatile (\u0026#34;%0\u0026#34; :: \u0026#34;C\u0026#34;(n1)); //error: S2::buf was not initialized at point of declaration asm volatile (\u0026#34;%0\u0026#34; :: \u0026#34;C\u0026#34;(S2::buf)); //error: buf4 was not initialized asm volatile (\u0026#34;%0\u0026#34; :: \u0026#34;C\u0026#34;(buf4)); } 8 位寄存器的约束 8 位寄存器没有特殊的字母来指定约束，但可以接收大于 8 位的类型。例如\n1 2 3 4 5 __device__ void copy_u8(char* in, char* out) { int d; asm(\u0026#34;ld.u8 %0, [%1];\u0026#34; : \u0026#34;=r\u0026#34;(d) : \u0026#34;l\u0026#34;(in) : \u0026#34;memory\u0026#34;); *out = d; } 会生成\n1 2 ld.u8 r1, [rd1]; st.u8 [rd2], r1; 约束中的修饰符 我们注意到前面的示例中有时在 “r” 前有 “=”，这个 “=” 修饰符表示这个寄存器用于输出。同时还有一个 “+” 修饰符表示这个寄存器同时被读出和写入。例如\n1 asm(\u0026#34;add.s32 %0, %0, %1;\u0026#34; : \u0026#34;+r\u0026#34;(i) : \u0026#34;r\u0026#34; (j)); 这里需要加这个约束的主要原因是，在实际执行时还需要有加载和存储字符串的过程。如对于这个 PTX 语句\n1 asm(\u0026#34;add.s32 %0, %1, %2;\u0026#34; : \u0026#34;=r\u0026#34;(i) : \u0026#34;r\u0026#34;(j), \u0026#34;r\u0026#34;(k)); 实际编译器的输出是\n1 2 3 4 ld.s32 r1, [j]; ld.s32 r2, [k]; add.s32 r3, r1, r2; st.s32 [i], r3; 所以约束在这里变得很重要，约束中作为输入的操作符会在实际语句执行前被加载进寄存器，而结果会在实际语句后被写入寄存器。\n使用 ASM 命令的一些问题 下面讨论一下使用 ASM 命令可能会遇到的一些问题。\n命名空间冲突 对于上面我们举例过的 cube 函数：\n1 2 3 4 5 6 7 8 9 __device__ int cube (int x) { int y; asm(\u0026#34;.reg .u32 t1;\\n\\t\u0026#34; // temp reg t1 \u0026#34; mul.lo.u32 t1, %1, %1;\\n\\t\u0026#34; // t1 = x * x \u0026#34; mul.lo.u32 %0, t1, %1;\u0026#34; // y = t1 * x : \u0026#34;=r\u0026#34;(y) : \u0026#34;r\u0026#34; (x)); return y; } 如果我们把它写成 inline 并在代码中多次使用，我们会受到一个报错，说临时寄存器 t1 重复声明。这时我们可以：\n不要 inline 这个函数 把使用 t1 的部分用大括号包起来，如： 1 2 3 4 5 6 7 8 9 10 11 __device__ int cube (int x) { int y; asm(\u0026#34;{\\n\\t\u0026#34; // use braces for local scope \u0026#34; reg .u32 t1;\\n\\t\u0026#34; // temp reg t1, \u0026#34; mul.lo.u32 t1, %1, %1;\\n\\t\u0026#34; // t1 = x * x \u0026#34; mul.lo.u32 %0, t1, %1;\\n\\t\u0026#34; // y = t1 * x \u0026#34;}\u0026#34; : \u0026#34;=r\u0026#34;(y) : \u0026#34;r\u0026#34; (x)); return y; } 内存空间冲突 asm() 语句无法得知传进来的寄存器在哪个内存空间里，所以用户需要确定使用了合适的 PTX 指令。在 sm_20 和以上版本中，所有传给 asm() 的指针都是 generic address。\n不正确的优化 一般直接使用 asm()，编译器会认为这句话唯一的作用是改变了输出变量，不会有其它作用，所以有时编译器会对这些语句进行优化。而使用 asm volatile() 会确保这个语句在生成 PTX 时不会被删除或移动顺序。如\n1 asm volatile (\u0026#34;mov.u32 %0, %%clock;\u0026#34; : \u0026#34;=r\u0026#34;(x)); 此外，内存操作也是类似。一般认为被写入的内存都会被放在输出操作符的位置上。但是如果有隐藏的内存读写（比如间接访存时的地址计算），或者我们就是想在生成 PTX 时去掉 asm() 语句附近的内存优化，我们可以在语句中增添第三个冒号，并加上 “memory”。例如\n1 2 asm volatile (\u0026#34;mov.u32 %0, %%clock;\u0026#34; : \u0026#34;=r\u0026#34;(x) :: \u0026#34;memory\u0026#34;); asm (\u0026#34;st.u32 [%0], %1;\u0026#34; :: \u0026#34;l\u0026#34;(p), \u0026#34;r\u0026#34;(x) : \u0026#34;memory\u0026#34;); 不正确的 PTX 由于编译器是不会检查 asm() 语句的内部的，所以错误只会在 ptxas 中显示。\n","date":"2025-03-25T14:23:56+08:00","permalink":"https://georgelyu.github.io/p/ptx/","title":"在 CUDA 中使用 PTX"},{"content":"一直感觉没有系统地梳理过设计模式，依据Design patterns for humans和一个非常好的翻译版，我在这里简要地梳理一下这些设计模式的本质。\n概述 最早的设计模式中有一些其实可能在现在已经太普遍了很难让人觉得是设计模式，或者有一些可能会被认为只是对一类编程技巧的称呼，只是因为用到了 OOP 特性变成了设计模式，还有一些甚至已经被语言吸收变成语言特性了。所以在这里先进行一个总结性的分类。\n创建型： 替代复杂的构造函数或构造逻辑：工厂、抽象工厂、生成器； 已经成为语言特性：原型（C++ 中的拷贝构造和拷贝赋值）； 一些特定场景下需要的编程技巧：单例（一个只能有一个实例的类）； 结构型： 通过添加中间层来实现额外功能：适配器（通过添加中间层类让新类兼容旧接口）、外观（通过添加中间层类简化复杂接口）、代理（通过添加中间层类替换原先的类，实现添加功能或其它目的）； 在接口的子类过多时，通过剥离新的抽象维度，并以组合而非继承的形式降低耦合度：桥接； 同一接口的子类通过互相拥有来形成层级结构和递归的操作：组合、装饰； 一些特定场景下需要的编程技巧：享元（将大量对象中重复存储的数据抽出单独存储，通常存储在哈希表中）； 行为模式： 将具体逻辑交给子类，通过子类的变化来完成不同的行为：策略（具体逻辑在 Strategy 类中，一般描述同一件事的不同完成方式，由 Caller 类调用）、状态（具体逻辑在 State 类中，来源是状态自动机的状态变化，与策略相比 Strategy 类一般互相是独立的，彼此不知道相互的存在，而 State 类之间一般知道互相的存在，甚至会发生互相的转变）、命令（具体逻辑在 Command 类中，Command 类对象可以被放在队列中或交给其它类执行，从而灵活地实现行为，可认为是 Strategy 类的更灵活的扩展）； 通过依赖注入与回调实现控制反转与解耦：中介者（所有的类都与一个中心的 Mediator 类沟通，由 Mediator 类操纵其它类的行为，目的是减少各类之间错综复杂的依赖关系）、观察者（消息订阅的范式，除了用来做消息订阅之外还可以用来实现中介者模式，但是不像中介者模式那样要求一个强中心类的存在，可以多个发布者和订阅者之间直接沟通）、访问者（每个类可以接受一个 Visitor 类的对象，这个对象包含了对这些类的操作，从而实现对类的行为的方便的扩展） 已经成为语言特性：迭代器（C++ 中的迭代器）； 一些特定场景下需要的编程技巧：责任链（将对象形成链表，然后通过顺序访问来决定行为顺序）、备忘录（让一个类在不暴露实现细节的情况下保存和恢复其状态）、模板方法（用接口来规定算法通用的部分，让子类实现不同的特定步骤）。 简单类 创建型（Creational） 生成器（Builder） 当我们有下面这样一个复杂的构造函数时，我们会发现这样的构造形式是不可持续的。\n1 Burger(int size, bool cheese = true, bool peperoni = true, bool tomato = false, bool lettuce = true); 我们可以用一个生成器来完成这个过程：\n1 Burger* burger = Burger::BurgerBuilder(14).AddPepperoni().AddLettuce().AddTomato().Build(); 这里的本质是将构造函数中的参数列表方法化了。\n原型（Prototype） 通过克隆, 基于已有对象来创建对象。本质就是 C++ 的拷贝构造和拷贝复制。\n结构型（Structural） 适配器（Adapter）/ 外观（Facade）/ 代理（Proxy） 这三个模式本质都是通过中间加一层来实现额外功能，适配器模式是为了实现不同类之间的兼容，外观模式是为了简化复杂接口，代理模式是为了增强一个类的功能。\n适配器模式的本质是如果一些代码只接受某个类 A 作为参数，当我们需要增加一个新类 B 来兼容这些操作时，可以用一个 Adapter 类（中间层）来作为原先类 A 的子类，然后用类 B 的逻辑来实现。比如 microSD 需要一个 SD 转换器来插入电脑的 SD 接口。这个的重点是，客户端还会认为自己在使用类 A。 外观模式的本质是把复杂的业务逻辑包装成一个单独的 Facade 类（中间层）和对应的简单方法，比如把电脑开机的过程只包装成一个 Computer 类和 TurnOn() 方法，在内部执行起电、自检、调用加载动画等一系列逻辑。 代理模式可以被视为放松要求的适配器，只是适配器是严格遵守了需要被兼容的那个类（类 A）的接口，代理模式没有这种要求，只是接口类似于原先的接口。当然也许这两个类的接口可以被一个抽象类确定，从而这两个类（类 A 和 Proxy 类）的接口可以完全一致，这样更容易兼容。代理模式和适配器模式最根本的区别是，客户端是明确地知道自己在使用 Proxy 类（中间层）来执行业务逻辑的。 享元（Flyweight） 如果类 A 的多个对象拥有相同的状态，可以把这些状态抽出来变成一个新的类 SharedState，这样原先类 A 的多个对象可以指向同一个 SharedState 对象即可，从而节省内存。这个模式称为享元。一般共享的这个 SharedState 可能会存储在哈希表里方便查找。\n行为模式（Behavioral） 责任链（Chain of Responsibility） 责任链有点像在学校请假。请假需求先找导师签字，导师通过行政老师签字，行政老师通过院长签字，等等。如果某一个环节打回来了，那就重新来。就像这样的一个对象一个对象检查过去的模式，每个对象可以选择自己处理，或者交给责任链上的下个责任人（当然，也可以都做了，比如上面的自己签字然后再交下去）。实现形式是所有类都继承同一个接口，每个类都有一个该接口的指针，指向链上的下一个对象。\n当然，话说回来了，如果没有这样一个指针，把这些对象按顺序放在一个 vector 里，那效果也是一样的。只能说在具体实现的时候要足够灵活。\n迭代器（Iterator） C++ 语言特性。如果用 std 容器，这个迭代器本身就包含了。但是很多时候为了效率，迭代器并不是很好的方案。它的目的是包装访问容器内元素的方法，但是这个方法又有什么好包装的呢……\n备忘录（Memento） 备忘录模式的本质就是缓存这个行为的对象化。比如我们现在想要把一些类当前的状态存下来，我们应该怎么办呢，类里面的很多数据可能是 private，是访问不到的。这时我们就应该创建一个 Memento 类，这个类是由 Originator 类（也就是我们想要保存其状态的类）创建的，因为它自己当然能访问自己的数据了。而 Originator 可以保存出来一个 Memento 类对象，也可以接收一个 Memento 类对象，来恢复自己的状态。而这个 Memento 类只需要对外暴露一些接口，也不必要把所有的数据暴露出来。\n模板方法（Template Method） 模板方法的本质是，大的业务逻辑是定死的，而小的业务逻辑可以改变。比如造房子必须遵循以下的步骤：\n建造地基； 砌墙； 建造屋顶。 但是这里面具体的实现可以变化，比如砌墙可以用石块，也可以用砖块。这些由具体的子类负责。相当于抽象类中确定了共同的逻辑，而可以变化的部分由子类进行具体的实现。\n重点关注类 创建型（Creational） 简单工厂（Simple Factory）/ 工厂（Factory）/ 抽象工厂（Abstract Factory） 简单工厂本质上只是提供了一个构造对象的接口。不使用构造函数，而是使用这个接口的理由是，有时使用构造函数之前步骤很多很复杂，为了提供一个简洁的接口，使用简单工厂。如\n1 IDoor* door = DoorFactory::MakeDoor(100, 200); 工厂则是适合用户需要不同的门的时候，可以调用不同的工厂，如\n1 2 3 4 5 DoorFactory* woodenDoorFactory = new WoodenDoorFactory(); woodenDoorFactory-\u0026gt;MakeDoor(100, 200); DoorFactory* ironDoorFactory = new IronDoorFactory(); ironDoorFactory-\u0026gt;MakeDoor(100, 200); 这时如果说 IDoor 类已经是一个抽象了，那么可以看到，这时抽象的维度从一维上升到了二维，多了一维创造对象方法的抽象。这层抽象使得我们之后想要扩展门的类型和创建方法的时候，可以更为简单。\n抽象工厂沿着这个思路就是抽象的维度又上升了一维，即不是一类型的门对应一类型的工厂，而是工厂本身也是抽象的。如一个工厂可能由好几个部门组成。造木门的需要木工和刷漆部门，但是造铁门除需要铁工外，也需要刷漆部门。对于一个可能维度有很多的对象来说，这样子我们也能通过组合来灵活地获得工厂，从而制造我们需要的对象。如\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 IDoorFactory* woodenFactory = new DoorFactory\u0026lt;WoodenDoor, Carpenter\u0026gt;(); { IDoor* door = woodenFactory-\u0026gt;MakeDoor(); IDoorFittingExpert* expert = woodenFactory-\u0026gt;MakeFittingExpert(); door-\u0026gt;GetDescription(); expert-\u0026gt;GetDescription(); } IDoorFactory* ironFactory = new DoorFactory\u0026lt;IronDoor, Welder\u0026gt;(); { IDoor* door = ironFactory-\u0026gt;MakeDoor(); IDoorFittingExpert* expert = ironFactory-\u0026gt;MakeFittingExpert(); door-\u0026gt;GetDescription(); expert-\u0026gt;GetDescription(); } 单例（Singleton） 一个类只会有一个对象的模式，非常适合存储需要全局存在并只允许存在一个的对象。\n结构型（Structural） 组合（Composite） 组合模式的目的是把对象组织成树状结构，这样我们调用最上层或中间某一层的对象使一系列的对象进行同样的操作。如我们需要调动军队，只需要告诉最高级的指挥官，他会命令他下属的指挥官们，下属的指挥官们再一级级向下传达。在程序中最常见的是 GUI 组件上的层级结构，如事件的捕捉和响应。代码实现参考这里。\n装饰（Decorator） 装饰模式很有意思，它就像那种俄罗斯套娃一样，所有的类都会继承自同一个接口，并且都有一个接受一个这个接口的构造函数。这个构造函数就相当于接收一个原来已经套了的娃，然后构造结束之后就又套了一层。用计算机的话说，类似递归的感觉。在这样的设计下，很多需要用递归来进行操作的逻辑就很好操作了。\n另外，这个模式和上面的组合有啥区别？我觉得……没啥区别，只是两种形式吧（笑）。\n桥接（Bridge） 桥接是在代码重构中会经常使用的模式。比如有一个抽象类，下面有几个子类，一直工作的也很好，但是随着系统的扩张，子类越来越多，这时我们发现这个数量已经很难再维护了，我们需要提升一些抽象的维度了。这时我们会对现有子类中一些共通的部分抽出来，并且不以继承，而以组合的形式给到原先的类。如下图：\n这个模式叫 bridge 的原因也是想要 “bridge a gap between the Abstraction and the Implementation”。我的理解就是单纯的抽象和子类这种写法只能在一些情况下满足我们的需求，有一些情况是不满足我们实际需求的。而这个问题，即 “gap”，怎么办呢，只能由桥接模式来 “bridge” 了。\n行为模式（Behavioral） 命令（Command） 命令模式还是值得好好学习一下的，它是把业务逻辑包装在一个 Command 接口类的子类中。负责调用的类会接收一个 Command 接口类对象，并执行其内在逻辑。举例，我们写一个软件有很多种方法来做同一种操作，比如复制我们可以点菜单栏中的按钮，可以用键盘上的 Ctrl+C，也可以右键菜单点击。这个时候我们是不希望把重复的逻辑来回写的。这个时候不同的 UI 控件或者响应就是不同的发送者（命令的发起者），如果命令很简单也许不需要接收者，命令直接就可以被执行。但是如果命令很复杂，也许命令要被发送到一个接收者位置，这个接收者来统一执行命令。\n用现实世界举例的话就是，在一个饭店里，可能有很多的服务员在响应不同的需求。有的顾客想要一杯水，服务员就会创造一个倒水的命令，但是这个命令可能很简单，服务员自己就直接执行了。而有的顾客点了一道菜，这个菜服务员是做不了的，它只能把命令移交到后厨，让后厨来处理。这里很多的服务员对应我们上个例子里的很多 UI 控件，后厨对应我们的接收者。\n当然，我们也可以统一把命令都交到同一个位置，这样的好处是方便我们记录和回滚，这也是我们在文本编辑器里经常用到的功能。\n中介者（Mediator） 中介者放在这里是因为它和上面的命令有点像。还是拿饭馆举例，命令模式里面的每个命令都像一个小纸条，记着客户的需求，当然每个小纸条都有它应该去的去处。比如按上面说的，点菜的命令送到后厨了，倒水的命令可能自己解决了，结账的命令可能要送到前台了。命令都是这样从发送者发起到接收者接收。而中介者相当于加了一个前台的领班，所有服务员接到的所有的命令都交给这个领班，这个领班再下发，或者告诉其他人该怎么工作，其他人听指挥就好了。同时，命令一般是单向的，即从发送者发向接收者。但是中介者模式里面中介者是老大，他可以做广播等完成一些命令模式不太好完成的事情。\n这两种哪种模式更好，我觉得要根据适用情况。比如飞机的指挥，一般都是由塔台来统一收集情况，统一指挥。但是有时一个大空域里只有几个小飞机的时候，反而是目视飞行，互相直接沟通。毕竟这时设立一个塔台的成本就有点高了。\n策略（Strategy） 策略模式就像现在 std 的一些函数会接收一个 Policy，你可以指定这个函数是串行执行还是并行执行。类似地，如果我们要进行排序操作，我们是用快排还是冒泡，这本质上都是策略的选择。即一个 Context 类会接收或拥有一个 Strategy 类，这个 Strategy 类包装了具体执行的逻辑。\n策略模式和上面的命令其实看起来也是很像的，命令也是把具体执行的逻辑包装在一个叫 Command 的类里面。但是命令模式更灵活，一般这个命令可能会被发到别的类去执行，可能被放入队列里等待执行了，或者做了别的事情。而策略模式一般是针对于完成一件事情的不同方式，只是在同一个 Context 类进行一些方法的切换。\n状态（State） 状态模式就像状态自动机一样，用我们的手机举例，对于开机键，当手机处于解锁状态时，按一下是锁屏，按两下可能是打开相机，长按是关机。但当手机处于锁屏状态时，按下按键是解锁屏幕，长按仍然是关机。那么这时候解锁和锁屏就是两个不同的“状态”，在不同的状态下，对象的响应可能是不同的。\n所以具体的逻辑仍然是封装在不同的 State 类中，这跟之前的命令和策略都很像。但是显然策略之间是比较独立的，比如就像我们之前举的例子，可能是完成同一个计算的不同算法选择，它们互相之间没有什么联系。而状态就像状态自动机一样，比如锁屏时按键变成开锁，开锁时按键变回锁屏，状态与状态之间是互相知道的，并且会互相转变的。\n观察者（Observer） 一个 Publisher 接口可以拥有很多个 Oberserver 接口的子类对象，这样在 Publisher 发生变化时可以调用 Oberserver 接口中的某些函数，实现回调。\n和上面的中介者模式非常像，可以说在某些情况下，观察者模式可以实现中介者模式，这时中介者就是发布者。但是回想起我们在讨论中介者模式时举的关于飞机指挥的例子，除了拥有中心塔台的中介者模式，观察者模式也可以用来实现第二种各个飞机之间互相直接沟通来确定航路的模式，这时会分布式地有很多发布者和观察者。这也许是观察者和中介者的一个主要区别，即中介者一般是一个有强中心性质的类，所有其它对象都会和它建立联系，而观察者模式只是建立了不同类间的沟通，而没有要求有一个强中心角色的存在。\n访问者（Visitor） 把一个应用在多个类上的行为抽象成一个 Visitor 接口，并对不同的类有不同的 visit 方法，之后原先的类中增加一个 accept 方法，接收一个 Visitor 对象，并调用其 visit 方法。\n当然我们会发现，这些行为我们本身就可以在这些类的抽象类中增加一个接口，然后各个子类进行各自的实现即可。但是这样当我们要为各个类增加新的行为时，就需要改变原先的代码，并且每个行为都要这样改。\n而使用访问者模式的好处是只要原先的代码应用了访问者模式，新的行为就非常好扩展了，不用修改原先的代码。\n","date":"2025-03-21T16:00:12+08:00","permalink":"https://georgelyu.github.io/p/design_p_atterns/","title":"设计模式总结"},{"content":"在这里记录一下关于 softmax 函数的 CUDA 实现的优化，基本翻译自 Maharshi Pandya 的 Learning CUDA by optimizing softmax: A worklog 这篇博客。\nSoftmax 的定义 Softmax 函数的输入是一个有 $N$ 个元素的数组 $X = \\{ x_i \\}$，输出是同样的一个有 N 个元素的数组 $O = \\{ o_i \\}$，第 $i$ 个输出元素 $o_i$ 的定义如下：\n$$ o_i = \\frac{e^{x_i}}{\\sum_{k = 0}^{N - 1} e^{x_k}}. $$这里能看到对于每个输入元素 $x_i$，在计算时除了其自身的值，主要还需要一个所有元素的指数和。\n但是这里有一个问题，就是 $e^x$ 在 $x$ 较小时会很快地趋向于 0，在 $x$ 较大时则会很快地爆炸性增长。这对于浮点数的表示精度来说非常不好，即我们在使用 float 进行计算，且当 $x$ 有比较极端的值时，上面定义的 softmax 函数数值上并不稳定（分别会发生下溢和上溢）。\n举例来说，对于 $X = \\{3, 1, -3\\}$，我们直接计算可以得到结果 \\O = {0.88, 0.12, 0\\}。但是对于 $X = \\{1000, 1000, 1000\\}$，我们会得到 -nan，因为使用 float 表示时，exp(1000) = inf。同理对于 $X = \\{-1000, -1000, -1000\\}$ 也是一样的会得到 -nan，因为 -exp(1000) = 0。\n所以我们可以定义一个修改的 softmax 函数：把每个输入元素 $x_i$ 先减去数组中的最大值，即\n$$ o_i = \\frac{e^{x_i-x_{max}}}{\\sum_{k = 0}^{N - 1} e^{x_k-x_{max}}}. $$这样做的好处是保证了指数最大不会超过 0，所以不会发生上溢出（即不会得到 inf）。下溢出即使发生也没关系，下溢出的值被视为 0 不会影响我们得到一个合理的值。\n当然最后我们证明一下，我们修改过的版本和之前是等价的：\n$$ \\begin{split} \\frac{e^{x_i-x_{max}}}{\\sum_{k = 0}^{N - 1} e^{x_k-x_{max}}} \u0026= \\frac{e^{-x_{max}} \\cdot e^{x_i}}{e^{-x_{max}} \\cdot \\sum_{k = 0}^{N - 1} e^{x_k}} \\\\ \u0026= \\frac{e^{x_i}}{\\sum_{k = 0}^{N - 1} e^{x_k}} \\end{split} .$$接下来我们在计算时都会使用这个经过修改的版本。\n通常，在计算时，我们不会只对一个数组计算 softmax，而是对多个数组同时计算。我们假设有 $M$ 个这样的数组，每个数组有 $N$ 个元素，则我们有一个 $M \\times N$ 的矩阵作为输入，同时我们的输出也是一个 $M \\times N$ 的矩阵。\n第一版实现：naive 接下来我们进行 CUDA 的实现。我们首先令一个线程处理一行数据，即对于一个数组进行一个串行的实现。这时我们需要三轮计算，因为每一轮都依赖于上一轮得到的结果：\n计算 $x_{max}$； 计算 $\\norm = \\sum_{k = 0}^{N - 1} e^{x_k-x_{max}}$； 计算 $o_i = \\frac{e^{x_i-x_{max}}}{norm}$。 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 template \u0026lt;int kBlockDim\u0026gt; __global__ void MySoftMaxKernel(float* d_X, float* d_O, int M, int N) { int row = blockIdx.x * kBlockDim + threadIdx.x; if (row \u0026lt; M) { // max float m = -1 * INFINITY; // norm factor float L = 0.0f; // 3 passes (not optimal) for (int col = 0; col \u0026lt; N; col++) { int i = row * N + col; m = max(m, d_X[i]); } for (int col = 0; col \u0026lt; N; col++) { int i = row * N + col; L += expf(d_X[i] - m); } for (int col = 0; col \u0026lt; N; col++) { int i = row * N + col; d_O[i] = expf(d_X[i] - m) / L; } } } void MySoftMax(float* d_X, float* d_O, int kM, int kN) { constexpr int kBlockDim = 1024; dim3 block(kBlockDim); dim3 grid((kM + kBlockDim - 1) / kBlockDim); MySoftMaxKernel\u0026lt;kBlockDim\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_X, d_O, kM, kN); } 第二版实现：online softmax 我们先从算法角度进行一下优化，三轮显然计算有点多了，我们尝试能不能将第一轮（计算 $x_{max}$）和第二轮（计算 $norm$）进行融合。\n因为我们是一个一个处理元素的，在处理过程中，$x_{max}$ 和 $norm$ 会不断地得到更新。我们先处理第一个元素 $x_0$，此时\n$x_{max0} = x_0$ $norm_0 = e^{(x_0-x_{max0})}$ 这时我们处理下一个元素 $x_1$，如果这个元素比 $x_{max0}$ 小的话，我们就不用修改 $x_{max}$，直接增加 $norm$ 即可，即\n$x_{max1} = x_{max0}$ $norm_1 = norm_0 + e^{(x_1-x_{max1})}$ 但如果 $x_1$ 比先前的最大值 $x_{max0}$ 大，则之前的 $norm$ 计算是有问题的，必须进行修正（因为 $x_{max}$ 更新了）。\n这时我们对先前的 $norm0$ 乘一个修正项 $e^{(x_{max0} - x_{max1})}$，即可得到修正后的 $cnorm_0 = e^{(x_0-x_{max1})}$。所以这时我们得到了\n$x_{max1} = x_{1}$ $norm_1 = norm_0 \\cdot e^{(x_{max0} - x_{max1})} + e^{(x_1-x_{max1})}$ 实际上这已经变成了一个递推式了，我们将这个递推式写成代码为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 template \u0026lt;int kBlockDim\u0026gt; __global__ void MySoftMaxKernel(float* d_X, float* d_O, int M, int N) { int row = blockIdx.x * kBlockDim + threadIdx.x; if (row \u0026lt; M) { float m = -1 * INFINITY; float L = 0.0f; // compute max and norm factor in one pass only // by exploiting the property of exponentials for (int col = 0; col \u0026lt; N; col++) { int i = row * N + col; float curr = d_X[i]; if (curr \u0026gt; m) { // norm needs to be mutiplied by correction term L = L * expf(m - curr); m = curr; } L += expf(curr - m); } for (int col = 0; col \u0026lt; N; col++) { int i = row * N + col; d_O[i] = expf(d_X[i] - m) / L; } } } void MySoftMax(float* d_X, float* d_O, int kM, int kN) { constexpr int kBlockDim = 1024; dim3 block(kBlockDim); dim3 grid((kM + kBlockDim - 1) / kBlockDim); MySoftMaxKernel\u0026lt;kBlockDim\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_X, d_O, kM, kN); } 这样我们成功地省掉了一次 for 循环。\n第三版实现：使用并行 reduction 先前的算法虽然节省了一个 for 循环，但显然对于单个数组（$M = 1$）的情况，我们的实现依然是串行的。然而从上面我们 online softmax 的推导可以得到，我们可以对一个数组分段地进行 $x_{max}$ 和 $norm$ 计算，再进行归约（reduce）。（在上面的推导中，我们可以认为是用一个多个元素得到的结果和一个单个元素进行了归约计算，但是这个可以很 trivial 地推广至多个元素和多个元素的归约计算。）\n依据这个精神，我们可以用一个线程进行多个元素的 softmax 计算。为了保证读 global memory 的 coalescence，每个线程跨越 blockDim.x 去进行处理的，即线程 0 处理 {0, blockDim.x, blockDim.x * 2, \u0026hellip;}，线程 1 处理 {1, blockDim.x + 1, blockDim.x * 2 + 1, \u0026hellip;}。\n在每个线程算完自己部分的局部 $x_{max}$ 和 $norm$ 之后，我们需要进行一个并行归约。这个过程每个线程会先把自己的局部结果存至 shared memory 之中，然后进行并行归约，最终结果会存在 smem[0] 之中。（这里我们假设了输入的数据规模用一个 block 就可以完成这个归约。）在对 $x_{max}$ 和 $norm$ 都完成归约之后我们就可以对结果进行并行地计算了。\n参考代码如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 template \u0026lt;int kBlockDim\u0026gt; __global__ void MySoftMaxKernel(float* d_X, float* d_O, int M, int N) { // max and norm reduction will happen in shared memory (static) __shared__ float smem[kBlockDim]; int row = blockIdx.x; int tid = threadIdx.x; // edge condition (we don\u0026#39;t process further) if (row \u0026gt;= M) return; float* input_row = d_X + row * N; float* output_row = d_O + row * N; float local_max = -INFINITY; float local_norm = 0.0f; // compute local max and norm for each thread // and then finally have a sync barrier before moving on for (int i = tid; i \u0026lt; N; i += kBlockDim) { float x = input_row[i]; if (x \u0026gt; local_max) { local_norm *= expf(local_max - x); local_max = x; } local_norm += expf(x - local_max); } __syncthreads(); // each thread will have its own local max // we store it in the tid of the shared memory smem[tid] = local_max; __syncthreads(); // block-level reduction in O(log(N)) time over all threads // is faster than linear reduction over all threads for (int stride = kBlockDim / 2; stride \u0026gt; 0; stride /= 2) { if (tid \u0026lt; stride) { smem[tid] = max(smem[tid], smem[tid + stride]); } // sync barrier before next iteration to ensure correctness __syncthreads(); } // the first element after max reduction from all threads // will contain the global max for the row float row_max = smem[0]; __syncthreads(); // each thread will have its own local norm // we will store the corrected local norm in the shared memory // again, exploits property of exponentials smem[tid] = local_norm * expf(local_max - row_max); __syncthreads(); // sum reduction similar to above for global norm factor for (int stride = kBlockDim / 2; stride \u0026gt; 0; stride /= 2) { if (tid \u0026lt; stride) { smem[tid] += smem[tid + stride]; } __syncthreads(); } float row_norm = smem[0]; __syncthreads(); // finally, compute softmax for (int i = tid; i \u0026lt; N; i += kBlockDim) { output_row[i] = expf(input_row[i] - row_max) / row_norm; } } void MySoftMax(float* d_X, float* d_O, int kM, int kN) { constexpr int kBlockDim = 1024; dim3 block(kBlockDim); dim3 grid(kM); MySoftMaxKernel\u0026lt;kBlockDim\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_X, d_O, kM, kN); } 第四版实现：warp-level reduction 我们注意到在上面的 reduction 中，我们使用了很多的 block 级的 __syncthreads()，同时我们还需要使用很多的 shared memory。为了避免这些开销，我们可以使用 warp-level primitives 来完成 reduction。\n这些函数较为通用的写法是这样的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /* Takes in an array of size `TILE_SIZE` and reduces it as warp-wide sum. The first element in the array will contain the reduced sum. */ __device__ __forceinline__ float warpReduceSum(float val) { for(int offset = warpSize / 2; offset \u0026gt; 0; offset /= 2) { val += __shfl_down_sync(0xffffffff, val, offset); } return val; } /* Takes in an array of size `TILE_SIZE` and reduces it warp-wide max. The first element in the array will contain the reduced max. */ __device__ __forceinline__ float warpReduceMax(float val) { for (int offset = warpSize / 2; offset \u0026gt; 0; offset /= 2) { val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset)); } } 而在 kernel 中我们只需这样做：\n1 2 3 4 5 6 7 float local_max = ...; float max_reduce_result = warpReduceMax(local_max); ... float local_norm = ...; float norm_reduce_result = warpReduceSum(local_norm); 这样我们就完成了一个 warp 级的 reduction。不过在每个 warp 完成 reduction 之后，最终我们还是要对这些结果再进行一次 reduction。由于 warp-level reduction 的结果存在每个线程 0（warp 中编号）的寄存器中，所以想要让整个 block 拿到这个数据，我们还是要把这个结果放在 shared memory 中，然后再进行 reduction。如果这次的 reduction 的结果小于 32 个的话，我们甚至可以再进行一次 warp-level reduction。这里我们就做了这样的假设，连续使用了 warp-level reduction。\n参考代码如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 template \u0026lt;int kBlockDim\u0026gt; __global__ void MySoftMaxKernel(float* d_X, float* d_O, int M, int N) { // number of threads in a warp constexpr int kWarpSize = 32; // max and norm reduction will happen in shared memory (static) __shared__ float smem[(kBlockDim + kWarpSize - 1) / kWarpSize]; int row = blockIdx.x; int tid = threadIdx.x; if (row \u0026gt;= M) return; float* input_row = d_X + row * N; float* output_row = d_O + row * N; float local_max = -INFINITY; float local_norm = 0.0f; for (int i = tid; i \u0026lt; N; i += kBlockDim) { float x = input_row[i]; if (x \u0026gt; local_max) { local_norm *= expf(local_max - x); local_max = x; } local_norm += expf(x - local_max); } __syncthreads(); // warp level reduction using XOR shuffle (\u0026#39;exchanges\u0026#39; the values in the threads) // note: if there are 256 threads in one block (8 warps of 32 threads each) // the following for loop reduces the value in all the 8 warps // the 8 warps contain the 8 maximum values of the 32 threads that reside in those warps float val = local_max; for (int offset = kWarpSize / 2; offset \u0026gt; 0; offset /= 2) { val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset)); } // when blockDim is greater than 32, we need to do a block level reduction // AFTER warp level reductions since we have the 8 maximum values that needs to be // reduced again the global max will be stored in the first warp if (kBlockDim \u0026gt; kWarpSize) { if (tid % kWarpSize == 0) { // which warp are we at? // store the value in its first thread index smem[tid / kWarpSize] = val; } __syncthreads(); // first warp will do global reduction only // this is possible because we stored the values in the shared memory // so the threads in the first warp will read from it and then reduce if (tid \u0026lt; kWarpSize) { val = (tid \u0026lt; kBlockDim + (kWarpSize - 1) / kWarpSize) ? smem[tid] : -INFINITY; for (int offset = kWarpSize / 2; offset \u0026gt; 0; offset /= 2) { val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset)); } if (tid == 0) smem[0] = val; } } else { // this is for when the number of threads in a block are not // greater than the warp size, in that case we already reduced // so we can store the value if (tid == 0) smem[0] = val; } __syncthreads(); // we got the global row max now float row_max = smem[0]; __syncthreads(); // same reduction algorithm as above, but instead of max reduction // we do a sum reduction i.e. we accumulate the values val = local_norm * expf(local_max - row_max); for (int offset = kWarpSize / 2; offset \u0026gt; 0; offset /= 2) { val += __shfl_down_sync(0xffffffff, val, offset); } if (kBlockDim \u0026gt; kWarpSize) { if (tid % kWarpSize == 0) { smem[tid / kWarpSize] = val; } __syncthreads(); // first warp will do global reduction if (tid \u0026lt; kWarpSize) { val = (tid \u0026lt; kBlockDim + (kWarpSize - 1) / kWarpSize) ? smem[tid] : 0.0f; for (int offset = kWarpSize / 2; offset \u0026gt; 0; offset /= 2) { val += __shfl_down_sync(0xffffffff, val, offset); } if (tid == 0) smem[0] = val; } } else { if (tid == 0) smem[0] = val; } __syncthreads(); float row_norm = smem[0]; __syncthreads(); // finally, compute softmax for (int i = tid; i \u0026lt; N; i += kBlockDim) { output_row[i] = expf(input_row[i] - row_max) / row_norm; } } void MySoftMax(float* d_X, float* d_O, int kM, int kN) { constexpr int kBlockDim = 1024; dim3 block(kBlockDim); dim3 grid(kM); MySoftMaxKernel\u0026lt;kBlockDim\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_X, d_O, kM, kN); } ","date":"2025-03-14T14:57:27+08:00","permalink":"https://georgelyu.github.io/p/cuda_softmax_opt/","title":"CUDA Softmax 优化"},{"content":"CUDA shared memory bank conflict 是会造成 shared memory 访问延迟的一个原因。这篇主要介绍两种消除 CUDA bank confict 的方法，分别为 memory padding 与 swizzling。\n我们以矩阵转置为例，如果我们设定 block 的大小为 $32 \\times 32$，每个 block 对 GMEM 中这个矩阵 $32 \\times 32$ 的一小块进行转置。则这个 block 中的线程需要集体从 GMEM 中将矩阵搬运至 SMEM（不转置的搬运），并在转置后存储到 GMEM。注意在 SMEM 中，矩阵是没有转置的。所以在后面存储至 GMEM 时，我们需要一次读取 SMEM 中的一列数据，写入 GMEM 的一行中。\n可以看到这一过程在 shared memory 中会有很强的 bank confict，因为在转置后，所有的线程都在读同一列，而这里因为大小是 $32 \\times 32$，所以这一列都是同一个 bank。那么问题的根源就是破除掉这个整除的关系，即所有的线程因为整除的关系，全部被分配到了同一个 bank 进行读写。其中第一个方案是从分配空间的角度去考虑，即 memory padding。\nMemory padding Memory padding 是很简单地在开辟 shared memory 空间时，多开辟一个空间，如本来的代码为：\n1 __shared__ T shm[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_X]; 可以改为\n1 __shared__ T shm[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_X + 1]; 这样做可以令我们维持原有的序号进行访问，并错开实际的存储 bank。原理如下图：\nSwizzling Swizzling 的本质是改变线程到 shared memory 的索引，使得 bank 访问错开。比如对于我们上面的从 GMEM 读到 SMEM 的操作，我们可能是直接使用 threadIdx.y 和 threadIdx.x 来作为 shared memory 的索引的：\n1 2 3 4 5 6 7 8 size_t const input_matrix_from_idx_x{threadIdx.x + blockIdx.x * blockDim.x}; size_t const input_matrix_from_idx_y{threadIdx.y + blockIdx.y * blockDim.y}; size_t const input_matrix_from_idx{input_matrix_from_idx_x + input_matrix_from_idx_y * N}; size_t const shm_to_idx_x{threadIdx.x}; size_t const shm_to_idx_y{threadIdx.y}; shm[shm_to_idx_y][shm_to_idx_x] = input_matrix[input_matrix_from_idx]; 这样一列数据就都存到了 bank 0 中。然而我们可以使用一个异或来进行错开：\n1 2 3 4 5 6 7 8 9 size_t const input_matrix_from_idx_x{threadIdx.x + blockIdx.x * blockDim.x}; size_t const input_matrix_from_idx_y{threadIdx.y + blockIdx.y * blockDim.y}; size_t const input_matrix_from_idx{input_matrix_from_idx_x + input_matrix_from_idx_y * N}; size_t const shm_to_idx_x{threadIdx.x}; size_t const shm_to_idx_y{threadIdx.y}; size_t const shm_to_idx_x_swizzled{(shm_to_idx_x ^ shm_to_idx_y) % BLOCK_TILE_SIZE_X}; shm[shm_to_idx_y][shm_to_idx_x_swizzled] = input_matrix[input_matrix_from_idx]; 下图中每个方块中的数字就是 shm_to_idx_x_swizzled，也就是存储时的列号。可以看到，原本的第 0 列被存成了对角线，这样子我们再从 SMEM 中读出第 0 列时，我们会进行一个对角线读取，最小化 bank conflict（这个性质的证明在这里）。对其他列也是同理。\n","date":"2025-03-06T15:19:00+08:00","permalink":"https://georgelyu.github.io/p/cuda_bank_conflict/","title":"CUDA Bank Conflict 的解决方法"},{"content":"这是《Cooperative Groups: Flexible CUDA Thread Programming》这篇博客的学习笔记。\n动机 在 CUDA 中线程之间分享数据和协作工作是非常常见的。CUDA 为此提供了一个同步函数 __syncthreads()，但是这个函数只能在 block 间同步。有时我们会需要更细粒度的线程协作。\n所以 CUDA 推出了 Cooperative Groups programming model，这可以认为是原先 CUDA programming model 的一个扩展。\nCooperative Groups 基础 使用 Cooperative Groups 需要加头文件 #include \u0026lt;cooperative_groups.h\u0026gt;，并且所有的命名都在 cooperative_groups:: 命名空间下。\nCooperative Groups 中的基础类型是 thread_group，这是一个指向一组线程的 handle，这个 handle 只能被该组的线程访问。一个 group 有一些简单的接口，如 unsigned size() 来查询 group 内的线程数量，unsigned thread_rank() 来查询当前线程在 group 中的 id（在 0 到 size() - 1 之间）等。\n对于一个 group，可以用下面的语句来同步。\n1 2 g.sync(); // synchronize group g cg::synchronize(g); // an equivalent way to synchronize g 创建 group 很显然，我们不用自己创建，block 本身就符合一个 group 概念。所以我们可以通过\n1 thread_block block = this_thread_block(); 来拿到指向该 block 的 handle。我们对这个 group 同步的话就和之前的 __syncthreads() 是一样的，所以下面的所有语句作用是相同的。\n1 2 3 4 5 __syncthreads(); block.sync(); cg::synchronize(block); this_thread_block().sync(); cg::synchronize(this_thread_block()); thread_block 相比上面的 thread_group，多了\n1 2 dim3 group_index(); // 3-dimensional block index within the grid dim3 thread_index(); // 3-dimensional thread index within the block 这两个值，等同于先前的 blockIdx 和 threadIdx。\n想要把 group 继续细分，则可以使用 cg::tiled_partition() 函数，如我们可以用下面的代码把整个 block 分为 32 个线程的块，然后再分为 4 个线程一组的块：\n1 2 thread_group tile32 = cg::tiled_partition(this_thread_block(), 32); thread_group tile4 = tiled_partition(tile32, 4); 下面是一个 reduce sum 的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #include \u0026lt;cooperative_groups.h\u0026gt; using namespace cooperative_groups; __device__ int reduce_sum(thread_group g, int *temp, int val) { int lane = g.thread_rank(); // Each iteration halves the number of active threads // Each thread adds its partial sum[i] to sum[lane+i] for (int i = g.size() / 2; i \u0026gt; 0; i /= 2) { temp[lane] = val; g.sync(); // wait for all threads to store if(lane\u0026lt;i) val += temp[lane + i]; g.sync(); // wait for all threads to load } return val; // note: only thread 0 will return full sum } __device__ int thread_sum(int *input, int n) { int sum = 0; for(int i = blockIdx.x * blockDim.x + threadIdx.x; i \u0026lt; n / 4; i += blockDim.x * gridDim.x) { int4 in = ((int4*)input)[i]; sum += in.x + in.y + in.z + in.w; } return sum; } __global__ void sum_kernel_32(int *sum, int *input, int n) { int my_sum = thread_sum(input, n); extern __shared__ int temp[]; auto g = this_thread_block(); auto tileIdx = g.thread_rank() / 32; int* t = \u0026amp;temp[32 * tileIdx]; auto tile32 = tiled_partition(g, 32); int tile_sum = reduce_sum(tile32, t, my_sum); if (tile32.thread_rank() == 0) atomicAdd(sum, tile_sum); } 同时，对于 warp 来说，一个 warp 内的线程可能会发生 diverge，即 warp divergence。这时 SM 会用 active masks 来屏蔽没有激活的线程。而 Cooperative Groups 提供了 coalesced_threads() 函数来创建一个 coalesced threads group。\n1 2 3 4 5 6 7 auto block = this_thread_block(); if (block.thread_rank() % 2) { coalesced_group active = coalesced_threads(); ... active.sync(); } 很显然，最大的 coalesced threads group 就是一整个 warp。\n针对 warp 的优化 对齐到 warp 大小 我们可以把 group size 写到模板参数里，使用静态的 group 定义，这样 thread 的大小就在编译时已知了：\n1 2 thread_block_tile\u0026lt;32\u0026gt; tile32 = tiled_partition\u0026lt;32\u0026gt;(this_thread_block()); thread_block_tile\u0026lt;4\u0026gt; tile4 = tiled_partition\u0026lt;4\u0026gt; (this_thread_block()); 虽然我们可以随便定 group 的大小，但是当我们把 size 定到 warp 大小时，编译器会把同步做到 warp level，效率更高。\n使用 warp level 指令 同时我们可以使用下面的 warp level 指令来提速：\n1 2 3 4 5 6 7 8 9 .shfl() .shfl_down() .shfl_up() .shfl_xor() .any() .all() .ballot() .match_any() .match_all() ","date":"2025-03-04T14:26:36+08:00","permalink":"https://georgelyu.github.io/p/cuda_cooperative_groups/","title":"《Cooperative Groups Flexible CUDA Thread Programming》笔记"},{"content":"由于读博士期间不停地被浮点数精度所折磨，所以想要学习相关的内容。这里记录关于 posit 格式——作者认为可以用来替换 IEEE 754 浮点数的格式——的学习笔记。下面是 2017 年该论文的作者之一 Dr. John L. Gustafson 在斯坦福进行的演讲。该演讲是斯坦福 EE 计算机系统课程的一部分（实名羡慕）。\n背景 现有浮点数的问题 开篇作者便讲了现有浮点数最大的问题：计算的不稳定性。\nAnd there\u0026rsquo;s more。\n关于这里的最后一点，大多数实数无法被表示，其实这里想表达本质上现有的浮点数是一个 quantization，它只是把实数用靠近它的一个数值来表示。\n所以关于这一点，作者说他的想法是需要有一个 bit 来表达这个数是不是被精确表达了。就像我们写 $\\pi = 3.14$，这其实是不对的，但是我们如果在后面加上一个省略号，即 $\\pi = 3.14\\cdots$，那这就是正确的了，这表达了后面还有很多位数字我们无法表达。\n还有更多的问题。\nUnum Format Type 1 Unum 代表 universal number，是作者于 2015 年提出的一种格式。这个格式在 IEEE 754 上增加了三个 metadata，表示 ubit、指数和尾数的 size。\nUbit 带来的一个重要的区别是，相比之前的浮点数只能表达一个固定点，现在浮点数可以表达一个区间。\nUnum Format Type 2 作者接下来又提出了一种新的格式，令实数轴首尾相接，即正负无穷在一个点。同时一个好处是现在 x 和 1/x 是上下对称的（见下图），这使得乘法和除法之间的区别消失。\n但是这个方法的缺点是，用现有的电路来计算很难。所以作者使用了一种查表的方式进行计算。虽然这样也可以，但是作者还是想得到一种能不查表的方法。\nPosit 格式 Posit，作为单词的意思是“假设”，用作推定后面的事情。比如我们假设 A 是正确的，那么我们会有 B。这里的 A 就是一个假设。作者说因为之前他管这些叫“猜”（guesses），但是觉得需要一个更好的词（hhh）。\n在平均一个 number system 之前，我们先确定评价的标准，大概包括下面几个方面：\n","date":"2025-03-03T15:47:50+08:00","permalink":"https://georgelyu.github.io/p/posit_arithmetic/","title":"《Beyond Floating Point Next-Generation Computer Arithmetic》笔记"},{"content":"因为在实践过程中发现，occupancy 的高低似乎与性能没有绝对的联系，十分想探明背后的原因，所以进行了一番查询，找到了 Better Performance at Lower Occupancy 这个 talk（下面简称 talk）。这里是阅读并学习这一 talk 的笔记。\n惯性的思维 我们一般认为要想令 CUDA kernel 性能高，occupancy 需要够高，因为这样保证了每个 SM 上有足够的线程，从而可以 hide lantencies。但是从我的实际测试中看，事实并不是这样，这也是 talk 开篇就给出的事实：\n线程少也能掩盖计算延迟 延迟（latency）是执行一个操作所需的时间，如果是算数操作可能会是 ~20 个时钟周期，内存操作则可能是 ~400 个时钟周期。所以当某个操作正在进行时，依赖于该操作的其他操作会被堵塞。想要掩盖这种延迟，可以通过执行其它操作来掩盖：\n这里要注意延迟与吞吐量（throughput）的区别。\n延迟是单个操作的速率，比如算数操作比内存操作快一百倍：算术操作需要 4 个时钟周期，而内存操作需要 400 个时钟周期，这里指的是延迟。 吞吐量则是每个周期能完成多少操作。如算数操作的吞吐量是 1.3 TFLOPS，假设我们的 GPU 的时钟速度是 1.386 GHz，那么我们可以算出这个吞吐量相当于 480 ops/cycle（这里一个 op 对应一次乘加，即两次 FLOP，算法是 $(1.3 * 1024) \\text{ GFLOPS } / 2 / 1.386 \\text{ GHz } \\approx 480 \\text{ ops/cycle}$）。同理，如果内存操作吞吐量是 177 GB/s，则可以计算出这个吞吐量相当于 32 ops/cycle（这里一个 op 对应一次 32-bit load，算法是 $177 \\text{ GB/s } / 1.386 \\text{ GHz } / 4 \\text{ B } \\approx 32 \\text{ ops/cycle}$）。 虽然可以通过等待延迟时做其它操作来掩盖这一延迟，但是我们要注意到，整体的速度再快也不可能超过极限。这个极限可以用利特尔法则（Little\u0026rsquo;s law）来计算。\n利特尔法则的内容是：在一个稳定的系统中，长期的平均顾客人数（$L$），等于长期的有效抵达率（$\\lambda$），乘以顾客在这个系统中平均的等待时间（$W$）；或者，我们可以用一个代数式来表达：$L = \\lambda W$。\n这一公式可以用来描述一个商店中顾客长期的平均人数：如果顾客的到达率 $\\lambda$ 为每小时 10 人，平均每个顾客逛商店的时间 $W$ 是 0.5 小时，则商店中平均的顾客人数 $L$ 为 $10 \\times 0.5 = 5$ 人。\n利特尔法则也可以用来描述一个应用程序的响应时间：$L$ 为平均工作数量，$\\lambda$ 为平均吞吐量，$W$ 为平均响应时间（延迟）。\n那么就很清楚了，我们所需要的并行程度（parallelism，有多少操作在这个系统中）= 延迟 * 吞吐量：\n根据上图，我们要达到 100% 的吞吐量，就必须满足足够的并行度。图中一个 SM 有 8 个核，一个操作需要 24 个周期，则我们至少需要在一个 SM 上进行 $8 \\times 24 = 192$ 次操作。如果数量不足，则我们得不到 100% 的吞吐量，有些时钟周期就会空转。（这里的理解是指令的执行是流水线的，所以每一个时钟周期都需要送新的指令进去，来满足这个流水线的执行。）\n一般来说我们可以通过塞足够的线程来完成这一目标：\n但是我们也可以通过单个线程中指令的并行来完成这一点：\n举例对于下面的代码是没有 ILP 的情况：\n1 2 3 4 #pragma unroll UNROLL for( int i = 0; i \u0026lt; N_ITERATIONS; i++ ) { a = a * b + c; } 在 N_ITERATIONS 足够大， a、b、c 都在寄存器中，且 a 之后会被用到的情况下，这样的代码在 GTX 480 上需要 576 个线程来达到 100% 的算数吞吐量。\n而在 ILP = 2 时，下面的代码\n1 2 3 4 5 #pragma unroll UNROLL for( int i = 0; i \u0026lt; N_ITERATIONS; i++ ) { a = a * b + c; d = d * b + c; } 在实验中仅需要 320 个线程来达到 100% 的算数吞吐量。这证明了线程数量（TLP）之外，ILP 同样可以贡献算数吞吐量。最终直到 ILP = 4 后，ILP 能带来的贡献达到极限，仅需要 192 个线程。\n所以总结：\n增加 occupancy 并不是唯一可以进行 latency hiding 的方法，增加 ILP 程度也是。 Occupancy 不是衡量硬件利用率的指标，它只是其中的一个 contributing factor。 为了完全掩盖计算延迟，并不需要一定让线程数量跑满（即最大 occupancy）。 线程少也能掩盖内存延迟 同理，对于内存延迟也是类似的。如下图的例子中，经过计算我们如果要掩盖掉内存延迟，需要时刻读取 100 KB。\n那么我们可以用很多种方法来做到时刻读取 100 KB：\n可以增加线程数。 可以用 ILP（每一个线程增加互不依赖的访存）. 可以用 bit-level parallelism（使用 64 位或 128 位的访存） 举例，对于下面的代码\n1 2 3 4 5 6 7 __global__ void memcpy(float *dst, float *src) { int block = blockIdx.x + blockIdx.y * gridDim.x; int index = threadIdx.x + block * blockDim.x; float a0 = src[index]; dst[index] = a0; } occupancy 与内存吞吐量之间的关系为：\n而当我们增加拷贝 float 的数量时\n1 2 3 4 5 6 7 8 9 10 11 __global__ void memcpy(float *dst, float *src) { int iblock = blockIdx.x + blockIdx.y * gridDim.x; int index = threadIdx.x + 2 * iblock * blockDim.x; float a0 = src[index]; // no latency stall float a1 = src[index+blockDim.x]; // stall due to data dependency dst[index] = a0; dst[index+blockDim.x] = a1; } 达到同样内存吞吐量需要的 occupancy 就会下降。如果我们增加到拷贝 4 个 float\n1 2 3 4 5 6 7 8 9 __global__ void memcpy(float *dst, float *src) { int iblock = blockIdx.x + blockIdx.y * gridDim.x; int index = threadIdx.x + 4 * iblock * blockDim.x; float a[4];//allocated in registers for(int i=0;i\u0026lt;4;i++) a[i]=src[index+i*blockDim.x]; for(int i=0;i\u0026lt;4;i++) dst[index+i*blockDim.x]=a[i]; } 则会变成这样：\n我们还可以把拷贝 float 变成拷贝 float2 或 float4 来进一步降低需要的 occupancy：\n所以，与掩盖计算延迟同理，想要掩盖内存延迟，可以提升 occupancy，也可以提升每个线程访存的量。\n总结：低 occupancy 与低线程数并不绝对地意味着掩盖不了内存延迟，并不绝对地意味着内存吞吐量低。\n用更少的线程让程序更快 首先我们要理解寄存器的重要性。比如对于一个式子 $a * b + c$，需要两个浮点操作，加载 3 个浮点数，并输出 1 个浮点数。如果算数吞吐量是 1.3 TFLOPS，则加载这个浮点数的内存吞吐量至少需要 $1.3\\text{ TFLOPS} / 2 * 12\\text{ B} = 7.8 \\text{ TB/s}$（我不知道下面这个 8.1 TB/s 作者是咋算出来的）。但是 shared memory 不能提供这么高的吞吐量。\n而只有寄存器可以支持这么高的内存吞吐量：\n而如果需要给每个线程分配更多的寄存器，则线程数量就需要减少，occupancy 也会随之减少，但是每一次计算所需的内存吞吐量可以获得提升。\n这也解释了 thread coarsening 能获得性能提升的一个重要理由：一个线程负责更多的输出，可以使得更多的数据被搬到寄存器上，shared memory 的访问也会减少，从而提升算数计算的效率。\nCase study：GEMM 请参考 GEMM 优化这一篇文章。\n","date":"2025-02-23T13:02:52+08:00","permalink":"https://georgelyu.github.io/p/lower_occupancy_note/","title":"《Better Performance at Lower Occupancy》笔记"},{"content":"因为最近读完了《Programming Massively Parallel Processors: A Hands-on Approach (4th Edition)》这本书（下面简称 PMPP），非常想结合书中的内容实操一下。\n所以我结合了 cuda 入门的正确姿势：how-to-optimize-gemm 和 How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog 这两篇非常好的文章，自己结合 PMPP 中的经验写了一版代码，同时记录一下学习的过程。\n第一版实现：naive 第一版基本是最 naive 的实现。如果这个矩阵乘法是 $A \\times B = C$，$A$、$B$、$C$ 三个矩阵的维度分别为 $m \\times k$、$k \\times n$、$m \\times n$，则下面的 kernel 中每个线程对应矩阵 $C$ 中的一个元素，所以每个线程会进行 $k$ 次浮点数乘加。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #define A(i, j) d_A[(i) * k + (j)] #define B(i, j) d_B[(i) * n + (j)] #define C(i, j) d_C[(i) * n + (j)] // Naive version __global__ void MyMatMulKernel(float* d_A, float* d_B, float* d_C, int m, int n, int k) { int _m = blockIdx.y * blockDim.y + threadIdx.y; int _n = blockIdx.x * blockDim.x + threadIdx.x; if (_m \u0026lt; m \u0026amp;\u0026amp; _n \u0026lt; n) { Value_t c_value = 0; for (int _k = 0; _k \u0026lt; k; ++_k) c_value += A(_m, _k) * B(_k, _n); C(_m, _n) = c_value; } } void MyMatMul(float* d_A, float* d_B, float* d_C, int m, int n, int k) { constexpr int kBlockDim = 16; dim3 block(kBlockDim, kBlockDim); dim3 grid((n + kBlockDim - 1) / kBlockDim, (m + kBlockDim - 1) / kBlockDim); MyMatMulKernel\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, m, n, k); } 因为是第一版实现，所以没有在乎任何的参数设置（如 block 大小等）。同时说一下下面所有的测试中都使用 float 类型计算，并用我自己的 NVIDIA RTX 3090 来测试。这一版的性能可以达到大约 2.2 TFLOPS（根据参数该卡的理论性能为 35.58 TFLOPS）。\n并且注意到，这里的写法已经进行了 global memory 的 coalesced access。\n第二版实现：block tiling 第二版使用了 PMPP 第 5.4 和 5.5 节中提到的 tiling 方法，即将会被重复使用的数据放置在 block 的 shared memory 中，这样减少了 global memory 的重复传输。这个思想也是对应原 repo 中的 MMult_cuda_3.cu。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #define A(i, j) d_A[(i) * k + (j)] #define B(i, j) d_B[(i) * n + (j)] #define C(i, j) d_C[(i) * n + (j)] // Tiled version template \u0026lt;int kTileWidth\u0026gt; __global__ void MyMatMulKernel(float* d_A, float* d_B, float* d_C, int m, int n, int k) { __shared__ Value_t Ads[kTileWidth][kTileWidth]; __shared__ Value_t Bds[kTileWidth][kTileWidth]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int _m = by * kTileWidth + ty; int _n = bx * kTileWidth + tx; Value_t c_value = 0; /* k operations of multiply-add are divided into phases, each phase correspond to an * iteration of for-loop */ for (int ph = 0; ph \u0026lt; std::ceil((Value_t)k / kTileWidth); ++ph) { /* Collectively load data into shared memory */ if (_m \u0026lt; m) Ads[ty][tx] = A(_m, ph * kTileWidth + tx); else Ads[ty][tx] = 0; if (_n \u0026lt; n) Bds[ty][tx] = B(ph * kTileWidth + ty, _n); else Bds[ty][tx] = 0; // Make sure all threads in block finished loading data __syncthreads(); /* Do multiple-add */ for (int k = 0; k \u0026lt; kTileWidth; ++k) c_value += Ads[ty][k] * Bds[k][tx]; // Make sure all threads in block finished using shared memory, so that we can go into // next iteration __syncthreads(); } if (_m \u0026lt; m \u0026amp;\u0026amp; _n \u0026lt; n) C(_m, _n) = c_value; } void MyMatMul(float* d_A, float* d_B, float* d_C, int m, int n, int k) { constexpr int kTileWidth = 16; dim3 block(kTileWidth, kTileWidth); dim3 grid((n + kTileWidth - 1) / kTileWidth, (m + kTileWidth - 1) / kTileWidth); MyMatMulKernel\u0026lt;kTileWidth\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, m, n, k); } 这里的写法大部分是直接用了 PMPP 书中 kernel 的写法。这一版可以达到大概 2.9 TFLOPS（小小进步）。\n我们这里可以简单计算一下，每个线程的访存（只考虑读）次数：\nGMEM：K / kTileWidth * 2 loads，代入数值（kTileWidth = 16）得到 K / 8 loads SMEM：K * 2 loads 这里因为每个线程输出一个结果元素，所以每个结果元素的访存次数和每个线程的访存次数是相等的。\n第三版实现：thread coarsening 这一版就比较有意思了，我们先来说一下应用的优化手法。手法是 PMPP 第 6.3 节中提到的 thread coarsening，即增加每个线程的颗粒度，不要使一个线程只负责一个元素。因为这种很细的颗粒度会导致很多的线程，所以会导致很多的 block。而当 block 或线程数很多的时候，在 GPU 中 block 之间会开始串行执行，这就增加了运行的 overhead。\n还是拿我的 RTX 3090 举例，当我们设置三个矩阵都是 $1024 \\times 1024$ 的方阵时，如果我们用最细颗粒度和 $16 \\times 16 = 256$ 的 block 大小（随意定的），那么我们会有 4096 个 block。\n然而查表可知，RTX 3090 每个 SM 最多支持 1536 个线程，16 个 block。因为我们的 block 大小是 256 个线程，所以由于每个 SM 上最大线程数（1536）的限制，一个 SM 只会分到 $1536 \\div 256 = 6$ 个 block。而 RTX 3090 一共只有 82 个 SM，所以总共的 4096 个 block 需要 $4096 \\div 6 \\div 82 \\approx 8.33$ 轮。这个轮在官方语言中称为 waves。在使用 Nsight Compute 进行 profiling 时，我们能看到这样一个属性就是“Waves per SM”，它的值也确实是我们计算的 8.33。\n可以想象这每一轮都会有并行计算的一些 overhead（如 block 的调度开销等），所以我们可以让一个线程多处理几个元素，从而减小 block 的大小，同时也减小了总线程数。这是我们的第三版实现，在第二版上增加了一个 stride 选项，控制一个线程会计算多少个元素。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 #define A(s, i, j) d_A[((i) + s * kTileWidthY) * k + (j)] #define B(s, i, j) d_B[((i) + s * kTileWidthY) * n + (j)] #define C(s, i, j) d_C[((i) + s * kTileWidthY) * n + (j)] // Tiled version with thread coarsening template \u0026lt;int kTileWidthX, int kTileWidthY, int kStrideY\u0026gt; __global__ void MyMatMulKernel(float* d_A, float* d_B, float* d_C, int m, int n, int k) { __shared__ Value_t Ads[kTileWidthX][kTileWidthY * kStrideY]; __shared__ Value_t Bds[kTileWidthX][kTileWidthY * kStrideY]; int tx = threadIdx.x; int ty = threadIdx.y; int bx = blockIdx.x; int by = blockIdx.y; int _m = by * (kTileWidthY * kStrideY) + ty; int _n = bx * kTileWidthX + tx; Value_t c_value[kStrideY] = {0}; /* k operations of multiply-add are divided into phases, each phase correspond to an * iteration of for-loop */ for (int ph = 0; ph \u0026lt; std::ceil((Value_t)k / kTileWidthX); ++ph) { for (int s = 0; s \u0026lt; kStrideY; ++s) { /* Collectively load data into shared memory */ if (s * kTileWidthY + _m \u0026lt; m) Ads[ty + s * kTileWidthY][tx] = A(s, _m, ph * kTileWidthX + tx); else Ads[ty + s * kTileWidthY][tx] = 0; if (_n \u0026lt; n) Bds[ty + s * kTileWidthY][tx] = B(s, ph * kTileWidthX + ty, _n); else Bds[ty + s * kTileWidthY][tx] = 0; } // Make sure all threads in block finished loading data __syncthreads(); // if (by == 1 \u0026amp;\u0026amp; bx == 0 \u0026amp;\u0026amp; tx == 0 \u0026amp;\u0026amp; ty == 0) printf(\u0026#34;%f %f\\n\u0026#34;, _m, _n); for (int s = 0; s \u0026lt; kStrideY; ++s) for (int k = 0; k \u0026lt; kTileWidthX; ++k) /* Do multiple-add */ c_value[s] += Ads[ty + s * kTileWidthY][k] * Bds[k][tx]; // Make sure all threads in block finished using shared memory, so that we can go // into next iteration __syncthreads(); } for (int s = 0; s \u0026lt; kStrideY; ++s) if (s * kTileWidthY + _m \u0026lt; m \u0026amp;\u0026amp; _n \u0026lt; n) C(s, _m, _n) = c_value[s]; } void MyMatMul(float* d_A, float* d_B, float* d_C, int m, int n, int k) { constexpr int kTileWidthX = 16; constexpr int kTileWidthY = 8; constexpr int kStrideY = 2; dim3 block(kTileWidthX, kTileWidthY); dim3 grid((n + kTileWidthX - 1) / kTileWidthX, (m + (kTileWidthY * kStrideY) - 1) / (kTileWidthY * kStrideY)); MyMatMulKernel\u0026lt;kTileWidthX, kTileWidthY, kStrideY\u0026gt; \u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, m, n, k); } 在这份代码里，为了不显著地增加每个 block 的 shared memory 用量，我们将每个 block 实际负责的 tile 大小依然定为 $16 \\times 16$，而 kStrideY 被用来控制每个线程负责的元素数量，从而控制了 block 中的线程大小。注意到每个 block 的大小现在是 kTileWidthX * kTileWidthY。而为了保证 tile 大小仍是 $16 \\times 16$，我们要人为地保证 kTileWidthY * kStrideY = 16。所以显而易见地，如果我们将 kStrideY 设为 1，则它会回退到第二版实现；如果我们将kStrideY 设为 2、4、8 或 16，则可以进行我们上面讲的 thread coarsening。\n在这一版中，由于我进行了上面的计算，所以我发现，如果 stride 超过 2，比如 stride 为 4 时，每个 block 的大小为 64 个线程，这样每个 block 的线程就会过小，使得 SM 填不满。上面我们提到，一个 SM 中只能驻 16 个 block，所以在这种情况下只能驻 $16 \\times 64 = 1024$ 个线程。这会让 occupancy 掉至 $1024 \\div 1536 \\approx 66.7$%。所以这里我先令 stride 为 2，测试结果可以达到大约 4.1 TFLOPS（又一个小小进步）。\n分析：stride 调参 这里当然既然有 stride 这个参数，不妨试一下各个参数值的效果。我测试了四个不同的参数值结果如下：\nstride 1 2 4 8 16 计算吞吐量（TFLOPS） 2.9 4.1 5.1 6.0 3.8 Block 线程数 256 128 64 32 16 理论 Occupancy 100% 100% 66.67% 33.33% 33.33% 实际 Occupancy 95.75% 90.90% 59.86% 30.29% 29.42% 可以发现这里很“反逻辑”的一点是随着 occupancy 变低，效率反而越来越高，直到 stride 到达 16。stride 到达 16 时，一个 block 的线程数已经低于 32 了，而 warp 是以 32 个线程为一组的，所以很显然浪费了很多效率。但是在 stride 从 1 到 8 的时候效率是不断提升的。\n为了研究这个问题的原因，我搜到了一个 talk《Better Performance at Lower Occupancy》，我阅读的笔记放在这篇文章里。\n在这篇 talk 里，它的 case study 就是讲的 GEMM 的 thread coarsening，可以说和我这里的第三版的修改一模一样。在分析这个原因时，该 talk 中提到一个很重要的点，就是对于 GEMM 这样的 memory-intensive 的应用，计算单元的计算吞吐量是用不完的。RTX 3090 的一个 SM 的计算吞吐量是 $35.58\\text{ TFLOPS } \\div 82\\text{ SMs }\\approx 444.3\\text{ GFLOPS}$。但是可以看到，我们这里的每一次乘加，都需要访问两个浮点数。所以要跑满这个计算吞吐量，我们需要 $444.3\\text{ GFLOPS } \\div 2\\text{ ops } \\times 8\\text{ B } = 1777.2\\text{ GB/s }$。即使我们已经把元素整体地预先 load 到 shared memory 里面了，然而 shared memory 的带宽是 $32 \\text{ banks } * 4\\text{ B/bank } * 1.395\\text{ GHz }= 178.56\\text{ GB/s}$。很明显，shared memory 的带宽依然是瓶颈。\n回到我们这个例子，我们注意到在第三版代码中的第 44 行有一个 Bds[k][tx]，这个元素是会被重用的。所以这里就减少了 shared memory 的读写。\n同样算一下每个线程的访存（只考虑读）次数：\nGMEM：K / kTileWidth * stride * 2 loads，代入数值（kTileWidth = 16，stride = 8）得到 K loads SMEM：K * (stride + 1) loads，代入数值（kTileWidth = 16，stride = 8）得到 K * 9 loads 但是一个线程现在输出 8 个元素了所以每个结果元素需要的访存量变为：\nGMEM：K / 8 loads SMEM：K * 9 / 8 loads 当然我们要验证一下我们的想法对不对，在 Nsight Compute 中我们还能得到下面的一些性能指标：\nstride 1 2 4 8 16 每线程寄存器量 38 40 40 52 72 shared memory load 语句执行量 83.89 M 50.33 M 33.55 M 25.17 M 41.94 M FMA 利用率（% active cycle） 10.69 15.66 20.89 24.09 26.98 确实如我们所预料，shared memory 的加载数量在减小，同时 FMA 单元利用率在提升。\n第四版实现：2D thread coarsening 那么我们基本已经确定要进一步提升计算吞吐量，所以我们要进一步减少访存量（这里说的访存是包含 shared memory 的）。所以我们进一步加强 thread coarsening 的强度，令一个线程计算 8 * 8 个元素。所以我们抛弃第三版中的 kStride，用二维的 kThreadWorkDimX 和 kThreadWorkDimY，表示一个 thread 的工作范围。\n下面的实现参考了这里的源码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 #define A(i, j) d_A[(i) * k + (j)] #define B(i, j) d_B[(i) * n + (j)] #define C(i, j) d_C[(i) * n + (j)] // Tiled version with 2D thread coarsening template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kThreadWorkDimX, int kThreadWorkDimY, int kTileDimK\u0026gt; __global__ void MyMatMulKernel(float* d_A, float* d_B, float* d_C, int m, int n, int k) { // Introduce kTileDimK, so the tile is not square, otherwise it would be too large __shared__ Value_t Ads[kBlockWorkDimY][kTileDimK]; __shared__ Value_t Bds[kTileDimK][kBlockWorkDimX]; const int by = blockIdx.y; const int bx = blockIdx.x; // (thread_outer_x, thread_outer_y) is the thread\u0026#39;s outer position w.r.t. block. const int thread_outer_x = threadIdx.x % (kBlockWorkDimX / kThreadWorkDimX); const int thread_outer_y = threadIdx.x / (kBlockWorkDimX / kThreadWorkDimX); // Move tile to beginning of d_A\u0026#39;s row and d_B\u0026#39;s column d_A += by * kBlockWorkDimY * k; d_B += bx * kBlockWorkDimX; d_C += by * kBlockWorkDimY * n + bx * kBlockWorkDimX; // Calculate the indices that this thread will load into SMEM const int Ads_row = threadIdx.x / kTileDimK; const int Ads_col = threadIdx.x % kTileDimK; const int Bds_row = threadIdx.x / kBlockWorkDimX; const int Bds_col = threadIdx.x % kBlockWorkDimX; // The stride is for GMEM coalescing constexpr int thread_num_in_block = (kBlockWorkDimX * kBlockWorkDimY) / (kThreadWorkDimX * kThreadWorkDimY); const int strideA = thread_num_in_block / kTileDimK; const int strideB = thread_num_in_block / kBlockWorkDimX; // Register caches for Ads and Bds Value_t A_reg[kThreadWorkDimY] = {0.0}; Value_t B_reg[kThreadWorkDimX] = {0.0}; Value_t c_value[kThreadWorkDimY][kThreadWorkDimX] = {0}; /* k operations of multiply-add are divided into phases, each phase correspond to an * iteration of for-loop */ for (int ph = 0; ph \u0026lt; std::ceil((Value_t)k / kTileDimK); ++ph) { /* Collectively load data into shared memory */ for (int loadOffset = 0; loadOffset \u0026lt; kBlockWorkDimY; loadOffset += strideA) Ads[Ads_row + loadOffset][Ads_col] = A(Ads_row + loadOffset, Ads_col); for (int loadOffset = 0; loadOffset \u0026lt; kTileDimK; loadOffset += strideB) Bds[Bds_row + loadOffset][Bds_col] = B(Bds_row + loadOffset, Bds_col); // Make sure all threads in block finished loading data __syncthreads(); // Advance tile d_A += kTileDimK; // move kTileDimK columns to right d_B += kTileDimK * n; // move kTileDimK rows down // Calculate per-thread results for (int k = 0; k \u0026lt; kTileDimK; ++k) { // SMEM to registers for (int i = 0; i \u0026lt; kThreadWorkDimY; ++i) A_reg[i] = Ads[thread_outer_y * kThreadWorkDimY + i][k]; for (int i = 0; i \u0026lt; kThreadWorkDimX; ++i) B_reg[i] = Bds[k][thread_outer_x * kThreadWorkDimX + i]; for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; ++cx) c_value[cy][cx] += A_reg[cy] * B_reg[cx]; } // Make sure all threads in block finished using shared memory, so that we can go // into next iteration __syncthreads(); } // Write results to GMEM for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; ++cx) C(thread_outer_y * kThreadWorkDimY + cy, thread_outer_x * kThreadWorkDimX + cx) = c_value[cy][cx]; } void MyMatMul(float* d_A, float* d_B, float* d_C, int m, int n, int k) { // This is LOGICAL, not REAL block dim. constexpr int kThreadWorkDimX = 8; constexpr int kThreadWorkDimY = 8; // kBlockWorkDimX/kThreadWorkDimX and kBlockWorkDimY/kThreadWorkDimY must be exact // division. constexpr int kBlockWorkDimX = 8 * kThreadWorkDimX; constexpr int kBlockWorkDimY = 8 * kThreadWorkDimY; constexpr int kTileDimK = 8; dim3 block((kBlockWorkDimX * kBlockWorkDimY) / (kThreadWorkDimX * kThreadWorkDimY)); dim3 grid((n + kBlockWorkDimX - 1) / kBlockWorkDimX, (m + kBlockWorkDimY - 1) / kBlockWorkDimY); MyMatMulKernel\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kThreadWorkDimX, kThreadWorkDimY, kTileDimK\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, m, n, k); } 这个优化的关键在于第 60-70 行。通过 2D 的 thread coarsening，原理与 1D thread coarsening 类似，但是元素的重用可以更多。可以以 (kThreadWorkDimX + kThreadWorkDimY) 次 shared memory 加载进行 (kThreadWorkDimX * kThreadWorkDimY) 次乘加计算（图示过程参考该文的 Kernel 5 部分）和该文的第 1.3 节。\n这时每个线程需要的访存量（只考虑读）变为：\nGMEM：K / 8 (outer loop iters) * 2 (A+B) * 512/64 (sizeSMEM/numThreads) loads，为 K * 2 loads SMEM：K / 8 (outer loop iters) * 8 (kTileDimK) * (8 + 8) (kThreadWorkDimY + kThreadWorkDimX) loads，为 K * 16 loads 则每个结果元素需要的访存量（只考虑读）变为：\nGMEM：K / 32 loads SMEM：K / 2 loads 经过测试，这时性能已经来到了 15 TFLOPS，相比之前最高的 6 TFLOPS 可以说达到了质的飞跃。\n第五版实现：vectorized memory access 这一版主要做了两个改变：\n在数据从 GMEM 读到 SMEM 时，把 As 进行转置，这样子可以进行 SMEM 的合并读取； 在读写 GMEM 和 SMEM 时使用 float4，以使用向量化的指令（如 lde.e.128、stg.e.128、lds.128、sts.128等） 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 #define A(i, j) d_A[(i) * k + (j)] #define B(i, j) d_B[(i) * n + (j)] #define C(i, j) d_C[(i) * n + (j)] // Tiled version with 2D thread coarsening and vectorized memory access template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kThreadWorkDimX, int kThreadWorkDimY, int kTileDimK\u0026gt; __global__ void MyMatMulKernel(float* d_A, float* d_B, float* d_C, int m, int n, int k) { // Introduce kTileDimK, so the tile is not square, otherwise it would be too large // Here Ads is transposed __shared__ Value_t Ads_T[kTileDimK][kBlockWorkDimY]; __shared__ Value_t Bds[kTileDimK][kBlockWorkDimX]; const int by = blockIdx.y; const int bx = blockIdx.x; // (thread_outer_x, thread_outer_y) is the thread\u0026#39;s outer position w.r.t. block. const int thread_outer_x = threadIdx.x % (kBlockWorkDimX / kThreadWorkDimX); const int thread_outer_y = threadIdx.x / (kBlockWorkDimX / kThreadWorkDimX); // Move tile to beginning of d_A\u0026#39;s row and d_B\u0026#39;s column d_A += by * kBlockWorkDimY * k; d_B += bx * kBlockWorkDimX; d_C += by * kBlockWorkDimY * n + bx * kBlockWorkDimX; // Calculate the indices that this thread will load into SMEM const int Ads_row = threadIdx.x / (kTileDimK / 4); const int Ads_col = threadIdx.x % (kTileDimK / 4); const int Bds_row = threadIdx.x / (kBlockWorkDimX / 4); const int Bds_col = threadIdx.x % (kBlockWorkDimX / 4); // The stride is for GMEM coalescing constexpr int thread_num_in_block = (kBlockWorkDimX * kBlockWorkDimY) / (kThreadWorkDimX * kThreadWorkDimY); const int strideA = thread_num_in_block / (kTileDimK / 4); const int strideB = thread_num_in_block / (kBlockWorkDimX / 4); // Register caches for Ads and Bds Value_t A_reg[kThreadWorkDimY] = {0.0}; Value_t B_reg[kThreadWorkDimX] = {0.0}; Value_t c_value[kThreadWorkDimY][kThreadWorkDimX] = {0}; /* k operations of multiply-add are divided into phases, each phase correspond to an * iteration of for-loop */ for (int ph = 0; ph \u0026lt; std::ceil((Value_t)k / kTileDimK); ++ph) { /* Collectively load data into shared memory */ for (int loadOffset = 0; loadOffset \u0026lt; kBlockWorkDimY; loadOffset += strideA) { // Here A is transpoed and saved into SMEM float4 tmp = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;A(Ads_row + loadOffset, Ads_col * 4))[0]; Ads_T[Ads_col * 4 + 0][Ads_row + loadOffset] = tmp.x; Ads_T[Ads_col * 4 + 1][Ads_row + loadOffset] = tmp.y; Ads_T[Ads_col * 4 + 2][Ads_row + loadOffset] = tmp.z; Ads_T[Ads_col * 4 + 3][Ads_row + loadOffset] = tmp.w; } for (int loadOffset = 0; loadOffset \u0026lt; kTileDimK; loadOffset += strideB) reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;Bds[Bds_row + loadOffset][Bds_col * 4])[0] = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;B(Bds_row + loadOffset, Bds_col * 4))[0]; // Make sure all threads in block finished loading data __syncthreads(); // Advance tile d_A += kTileDimK; // move kTileDimK columns to right d_B += kTileDimK * n; // move kTileDimK rows down // Calculate per-thread results for (int k = 0; k \u0026lt; kTileDimK; ++k) { // SMEM to registers // Due to transposed A in SMEM, the row and col indicies are switched for (int i = 0; i \u0026lt; kThreadWorkDimY; ++i) A_reg[i] = Ads_T[k][thread_outer_y * kThreadWorkDimY + i]; for (int i = 0; i \u0026lt; kThreadWorkDimX; ++i) B_reg[i] = Bds[k][thread_outer_x * kThreadWorkDimX + i]; for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; ++cx) c_value[cy][cx] += A_reg[cy] * B_reg[cx]; } // Make sure all threads in block finished using shared memory, so that we can go // into next iteration __syncthreads(); } // Write results to GMEM for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; cx += 4) reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;C(thread_outer_y * kThreadWorkDimY + cy, thread_outer_x * kThreadWorkDimX + cx))[0] = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;c_value[cy][cx])[0]; } void MyMatMul(float* d_A, float* d_B, float* d_C, int m, int n, int k) { // This is LOGICAL, not REAL block dim. constexpr int kThreadWorkDimX = 8; constexpr int kThreadWorkDimY = 8; // kBlockWorkDimX/kThreadWorkDimX and kBlockWorkDimY/kThreadWorkDimY must be exact // division. constexpr int kBlockWorkDimX = 8 * kThreadWorkDimX; constexpr int kBlockWorkDimY = 8 * kThreadWorkDimY; constexpr int kTileDimK = 8; dim3 block((kBlockWorkDimX * kBlockWorkDimY) / (kThreadWorkDimX * kThreadWorkDimY)); dim3 grid((n + kBlockWorkDimX - 1) / kBlockWorkDimX, (m + kBlockWorkDimY - 1) / kBlockWorkDimY); MyMatMulKernel\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kThreadWorkDimX, kThreadWorkDimY, kTileDimK\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, m, n, k); } 这一版的效率可以达到约 18 TFLOPS。\n第六版实现：warp tiling 在第四版实现时我们说了，现在一个 thread tile 可以用 (kThreadWorkDimX + kThreadWorkDimY) 次 shared memory 加载进行 (kThreadWorkDimX * kThreadWorkDimY) 次乘加计算。所以很明显，在乘积确定时，kThreadWorkDimX 和 kThreadWorkDimY 越接近，算存比会越高。\n那么对于一个 warp 的 32 个线程来说，这一过程是要进行 32 次的。我们可以认为每个 thread 在把数据从 SMEM 移至寄存器时，我们无形中是获得了 32 个 thread tile 在 warp 上。而这 32 个 thread tile 如果没有经过任何排布，比如是排成一行，那么可以想象，就和一个排成一行的 thread tile 一样，并没有最大化算存比。\n所以通过使用 warp tile，即把 warp 也做类似的 coarsening，我们能做到的一个重要的事情是，我们可以手动控制这个 warp 的 thread tile 的分布。所以我们可以通过控制维度来让 warp 内 thread tile 的排列变成尽可能方形的，来最大化算存比。\n同时我们注意到，在 kThreadWorkDimX = kThreadWorkDimY 时，这个值越大，算存比就越大。所以这样做给我们带来的另一个好处是，我们变相获得了一个很大的 thread tile。\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 #define A(i, j) d_A[(i) * k + (j)] #define B(i, j) d_B[(i) * n + (j)] #define C_interim(i, j) d_C_interim[(i) * n + (j)] // Tiled version with 2D thread coarsening, vectorized memory access and 2D warp tiling template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kWarpWorkDimX, int kWarpWorkDimY, int kWarpTileWorkDimX, int kWarpTileWorkDimY, int kThreadWorkDimX, int kThreadWorkDimY, int kBlockTileDimK, int kThreadNumInBlock\u0026gt; __global__ void MyMatMulKernel(float* d_A, float* d_B, float* d_C, int m, int n, int k) { // Introduce kTileDimK, so the tile is not square, otherwise it would be too large // Here Ads is transposed __shared__ Value_t Ads_T[kBlockTileDimK][kBlockWorkDimY]; __shared__ Value_t Bds[kBlockTileDimK][kBlockWorkDimX]; const int by = blockIdx.y; const int bx = blockIdx.x; constexpr int kWarpSize = 32; const int warpIdx = threadIdx.x / kWarpSize; // the warp this thread is in const int wy = warpIdx / (kBlockWorkDimX / kWarpWorkDimX); const int wx = warpIdx % (kBlockWorkDimX / kWarpWorkDimX); // thread\u0026#39;s position w.r.t. warp tile (ACTUAL warp). const int threadIdxInWarp = threadIdx.x % kWarpSize; const int ty_in_acutal_warp = threadIdxInWarp / (kWarpTileWorkDimX / kThreadWorkDimX); const int tx_in_acutal_warp = threadIdxInWarp % (kWarpTileWorkDimX / kThreadWorkDimX); // Move tile to beginning of d_A\u0026#39;s row and d_B\u0026#39;s column d_A += by * kBlockWorkDimY * k; d_B += bx * kBlockWorkDimX; // Here output position needs to go to warp output position d_C += (by * kBlockWorkDimY + wy * kWarpWorkDimY) * n + bx * kBlockWorkDimX + wx * kWarpWorkDimX; // Calculate the indices that this thread will load into SMEM const int Ads_row = threadIdx.x / (kBlockTileDimK / 4); const int Ads_col = threadIdx.x % (kBlockTileDimK / 4); const int Bds_row = threadIdx.x / (kBlockWorkDimX / 4); const int Bds_col = threadIdx.x % (kBlockWorkDimX / 4); const int strideA = kThreadNumInBlock / (kBlockTileDimK / 4); const int strideB = kThreadNumInBlock / (kBlockWorkDimX / 4); constexpr int kWarpTileNumY = kWarpWorkDimY / kWarpTileWorkDimY; constexpr int kWarpTileNumX = kWarpWorkDimX / kWarpTileWorkDimX; // Register caches for Ads and Bds (on the warptile level) Value_t A_reg[kWarpTileNumY][kThreadWorkDimY] = {0.0}; Value_t B_reg[kWarpTileNumX][kThreadWorkDimX] = {0.0}; // Results are in register file in warp scheduler Value_t c_value[kWarpTileNumY][kWarpTileNumX][kThreadWorkDimY][kThreadWorkDimX] = {0}; /* k operations of multiply-add are divided into phases, each phase correspond to an * iteration of for-loop */ for (int ph = 0; ph \u0026lt; std::ceil((Value_t)k / kBlockTileDimK); ++ph) { /* Collectively load data into shared memory */ for (int loadOffset = 0; loadOffset \u0026lt; kBlockWorkDimY; loadOffset += strideA) { // Here A is transpoed and saved into SMEM float4 tmp = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;A(Ads_row + loadOffset, Ads_col * 4))[0]; Ads_T[Ads_col * 4 + 0][Ads_row + loadOffset] = tmp.x; Ads_T[Ads_col * 4 + 1][Ads_row + loadOffset] = tmp.y; Ads_T[Ads_col * 4 + 2][Ads_row + loadOffset] = tmp.z; Ads_T[Ads_col * 4 + 3][Ads_row + loadOffset] = tmp.w; } for (int loadOffset = 0; loadOffset \u0026lt; kBlockTileDimK; loadOffset += strideB) reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;Bds[Bds_row + loadOffset][Bds_col * 4])[0] = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;B(Bds_row + loadOffset, Bds_col * 4))[0]; // Make sure all threads in block finished loading data __syncthreads(); // Advance tile d_A += kBlockTileDimK; // move kTileDimK columns to right d_B += kBlockTileDimK * n; // move kTileDimK rows down // Calculate per-thread results for (int k = 0; k \u0026lt; kBlockTileDimK; ++k) { // SMEM to registers // Due to transposed A in SMEM, the row and col indicies are switched for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int i = 0; i \u0026lt; kThreadWorkDimY; ++i) A_reg[w_sub_y][i] = Ads_T[k][wy * kWarpWorkDimY + w_sub_y * kWarpTileWorkDimY + ty_in_acutal_warp * kThreadWorkDimY + i]; for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) for (int i = 0; i \u0026lt; kThreadWorkDimX; ++i) B_reg[w_sub_x][i] = Bds[k][wx * kWarpWorkDimX + w_sub_x * kWarpTileWorkDimX + tx_in_acutal_warp * kThreadWorkDimX + i]; for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; ++cx) c_value[w_sub_y][w_sub_x][cy][cx] += A_reg[w_sub_y][cy] * B_reg[w_sub_x][cx]; } // Make sure all threads in block finished using shared memory, so that we can go // into next iteration __syncthreads(); } // Write results to GMEM for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) { // move C pointer to current warp subtile Value_t* d_C_interim = d_C + (w_sub_y * kWarpTileWorkDimY) * n + w_sub_x * kWarpTileWorkDimX; for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; cx += 4) reinterpret_cast\u0026lt;float4*\u0026gt;( \u0026amp;C_interim(ty_in_acutal_warp * kThreadWorkDimY + cy, tx_in_acutal_warp * kThreadWorkDimX + cx))[0] = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;c_value[w_sub_y][w_sub_x][cy][cx])[0]; } } void MyMatMul(float* d_A, float* d_B, float* d_C, int m, int n, int k) { // This is LOGICAL, not REAL block dim. constexpr int kNumOfThreads = 128; constexpr int kWarpSize = 32; constexpr int kNumOfWarps = kNumOfThreads / kWarpSize; constexpr int kThreadWorkDimX = 4; // in elemenets constexpr int kThreadWorkDimY = 8; // in elemenets // The work dim of one ACTUAL warp constexpr int kWarpTileWorkDimX = 4 * kThreadWorkDimX; // 16 (in elemenets) constexpr int kWarpTileWorkDimY = kWarpSize / (kWarpTileWorkDimX / kThreadWorkDimX) * kThreadWorkDimY; // 64 (in elemenets) // The work dim of one LOGICAL warp constexpr int kWarpWorkDimX = 4 * kWarpTileWorkDimX; // 64 (in elemenets) constexpr int kWarpWorkDimY = 1 * kWarpTileWorkDimY; // 64 (in elemenets) constexpr int kBlockWorkDimX = 2 * kWarpWorkDimX; // 128 (in elemenets) constexpr int kBlockWorkDimY = 2 * kWarpWorkDimY; // 128 (in elemenets) constexpr int kBlockTileDimK = 16; static_assert( (kBlockWorkDimX % kWarpWorkDimX == 0) \u0026amp;\u0026amp; (kBlockWorkDimY % kWarpWorkDimY == 0), \u0026#34;Block work dim is integral multiple of warp work dim.\u0026#34;); static_assert( (kBlockWorkDimX / kWarpWorkDimX) * (kBlockWorkDimY / kWarpWorkDimY) == kNumOfWarps, \u0026#34;Number of warps times its work dim should be equal to block work dim.\u0026#34;); static_assert((kWarpWorkDimY * kWarpWorkDimX) % (kWarpSize * kThreadWorkDimY * kThreadWorkDimX * kNumOfWarps) == 0, \u0026#34;Warp work dim is integral multiple of thread work dim.\u0026#34;); static_assert(kWarpWorkDimX % kNumOfWarps == 0, \u0026#34;Warp work dim is multiple of NUM_WARP\u0026#34;); static_assert((kNumOfThreads * 4) % kBlockTileDimK == 0, \u0026#34;NUM_THREADS*4 must be multiple of kTileDimK to avoid quantization \u0026#34; \u0026#34;issues during GMEM-\u0026gt;SMEM tiling (loading only parts of the \u0026#34; \u0026#34;final row of Bs during each iteraion)\u0026#34;); static_assert((kNumOfThreads * 4) % kBlockWorkDimX == 0, \u0026#34;NUM_THREADS*4 must be multiple of kBlockWorkDimX to avoid quantization \u0026#34; \u0026#34;issues during GMEM-\u0026gt;SMEM tiling (loading only parts of the \u0026#34; \u0026#34;final row of As during each iteration)\u0026#34;); static_assert(kBlockWorkDimX % (16 * kThreadWorkDimX) == 0, \u0026#34;kBlockWorkDimX must be a multiple of 16*kThreadWorkDimX to avoid \u0026#34; \u0026#34;quantization effects\u0026#34;); static_assert(kBlockWorkDimY % (16 * kThreadWorkDimY) == 0, \u0026#34;kBlockWorkDimY must be a multiple of 16*kThreadWorkDimY to avoid \u0026#34; \u0026#34;quantization effects\u0026#34;); static_assert((kBlockWorkDimY * kBlockTileDimK) % (4 * kNumOfThreads) == 0, \u0026#34;kBlockWorkDimY*kTileDimK must be a multiple of 4*kNumOfThreads to \u0026#34; \u0026#34;vectorize loads\u0026#34;); static_assert((kBlockWorkDimX * kBlockTileDimK) % (4 * kNumOfThreads) == 0, \u0026#34;kBlockWorkDimX*kTileDimK must be a multiple of 4*kNumOfThreads to \u0026#34; \u0026#34;vectorize loads\u0026#34;); dim3 block(kNumOfThreads); dim3 grid((n + kBlockWorkDimX - 1) / kBlockWorkDimX, (m + kBlockWorkDimY - 1) / kBlockWorkDimY); MyMatMulKernel\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kWarpWorkDimX, kWarpWorkDimY, kWarpTileWorkDimX, kWarpTileWorkDimY, kThreadWorkDimX, kThreadWorkDimY, kBlockTileDimK, kNumOfThreads\u0026gt;\u0026lt;\u0026lt;\u0026lt;grid, block\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, m, n, k); } 这一版的效率可以大概达到 20TFLOPS。\n第七版实现：函数化封装 为了接来下 double buffer 的实现，我们先把现在这一版实现做一下函数封装。代码如下（只包含 kernel 部分，因为调用部分不变）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 #define A(i, j) d_A[(i) * k + (j)] #define B(i, j) d_B[(i) * n + (j)] #define C_interim(i, j) d_C_interim[(i) * n + (j)] template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kBlockTileDimK, int strideA, int strideB\u0026gt; __device__ void LoadFromGmem(float* d_A, float* d_B, float Ads_T[kBlockTileDimK][kBlockWorkDimY], float Bds[kBlockTileDimK][kBlockWorkDimX], int n, int k, int Ads_col, int Ads_row, int Bds_row, int Bds_col) { /* Collectively load data into shared memory */ for (int loadOffset = 0; loadOffset \u0026lt; kBlockWorkDimY; loadOffset += strideA) { // Here A is transpoed and saved into SMEM float4 tmp = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;A(Ads_row + loadOffset, Ads_col * 4))[0]; Ads_T[Ads_col * 4 + 0][Ads_row + loadOffset] = tmp.x; Ads_T[Ads_col * 4 + 1][Ads_row + loadOffset] = tmp.y; Ads_T[Ads_col * 4 + 2][Ads_row + loadOffset] = tmp.z; Ads_T[Ads_col * 4 + 3][Ads_row + loadOffset] = tmp.w; } for (int loadOffset = 0; loadOffset \u0026lt; kBlockTileDimK; loadOffset += strideB) reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;Bds[Bds_row + loadOffset][Bds_col * 4])[0] = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;B(Bds_row + loadOffset, Bds_col * 4))[0]; } template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kWarpWorkDimX, int kWarpWorkDimY, int kWarpTileWorkDimX, int kWarpTileWorkDimY, int kThreadWorkDimX, int kThreadWorkDimY, int kBlockTileDimK, int kWarpTileNumY, int kWarpTileNumX\u0026gt; __device__ void ProcessFromSmem( float Ads_T[kBlockTileDimK][kBlockWorkDimY], float Bds[kBlockTileDimK][kBlockWorkDimX], float A_reg[kWarpTileNumY][kThreadWorkDimY], float B_reg[kWarpTileNumX][kThreadWorkDimX], float c_value[kWarpTileNumY][kWarpTileNumX][kThreadWorkDimY][kThreadWorkDimX], int wy, int wx, int ty_in_acutal_warp, int tx_in_acutal_warp) { // Calculate per-thread results for (int k = 0; k \u0026lt; kBlockTileDimK; ++k) { // SMEM to registers // Due to transposed A in SMEM, the row and col indicies are switched for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int i = 0; i \u0026lt; kThreadWorkDimY; ++i) A_reg[w_sub_y][i] = Ads_T[k][wy * kWarpWorkDimY + w_sub_y * kWarpTileWorkDimY + ty_in_acutal_warp * kThreadWorkDimY + i]; for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) for (int i = 0; i \u0026lt; kThreadWorkDimX; ++i) B_reg[w_sub_x][i] = Bds[k][wx * kWarpWorkDimX + w_sub_x * kWarpTileWorkDimX + tx_in_acutal_warp * kThreadWorkDimX + i]; for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; ++cx) c_value[w_sub_y][w_sub_x][cy][cx] += A_reg[w_sub_y][cy] * B_reg[w_sub_x][cx]; } } // Tiled version with 2D thread coarsening, vectorized memory access and 2D warp tiling template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kWarpWorkDimX, int kWarpWorkDimY, int kWarpTileWorkDimX, int kWarpTileWorkDimY, int kThreadWorkDimX, int kThreadWorkDimY, int kBlockTileDimK, int kThreadNumInBlock\u0026gt; __global__ void MyMatMulKernel(float* d_A, float* d_B, float* d_C, int m, int n, int k) { // Introduce kTileDimK, so the tile is not square, otherwise it would be too large // Here Ads is transposed __shared__ Value_t Ads_T[kBlockTileDimK][kBlockWorkDimY]; __shared__ Value_t Bds[kBlockTileDimK][kBlockWorkDimX]; const int by = blockIdx.y; const int bx = blockIdx.x; constexpr int kWarpSize = 32; const int warpIdx = threadIdx.x / kWarpSize; // the warp this thread is in const int wy = warpIdx / (kBlockWorkDimX / kWarpWorkDimX); const int wx = warpIdx % (kBlockWorkDimX / kWarpWorkDimX); // thread\u0026#39;s position w.r.t. warp tile (ACTUAL warp). const int threadIdxInWarp = threadIdx.x % kWarpSize; const int ty_in_acutal_warp = threadIdxInWarp / (kWarpTileWorkDimX / kThreadWorkDimX); const int tx_in_acutal_warp = threadIdxInWarp % (kWarpTileWorkDimX / kThreadWorkDimX); // Move tile to beginning of d_A\u0026#39;s row and d_B\u0026#39;s column d_A += by * kBlockWorkDimY * k; d_B += bx * kBlockWorkDimX; // Here output position needs to go to warp output position d_C += (by * kBlockWorkDimY + wy * kWarpWorkDimY) * n + bx * kBlockWorkDimX + wx * kWarpWorkDimX; // Calculate the indices that this thread will load into SMEM const int Ads_row = threadIdx.x / (kBlockTileDimK / 4); const int Ads_col = threadIdx.x % (kBlockTileDimK / 4); const int Bds_row = threadIdx.x / (kBlockWorkDimX / 4); const int Bds_col = threadIdx.x % (kBlockWorkDimX / 4); constexpr int strideA = kThreadNumInBlock / (kBlockTileDimK / 4); constexpr int strideB = kThreadNumInBlock / (kBlockWorkDimX / 4); constexpr int kWarpTileNumY = kWarpWorkDimY / kWarpTileWorkDimY; constexpr int kWarpTileNumX = kWarpWorkDimX / kWarpTileWorkDimX; // Register caches for Ads and Bds (on the warptile level) Value_t A_reg[kWarpTileNumY][kThreadWorkDimY] = {0.0}; Value_t B_reg[kWarpTileNumX][kThreadWorkDimX] = {0.0}; // Results are in register file in warp scheduler Value_t c_value[kWarpTileNumY][kWarpTileNumX][kThreadWorkDimY][kThreadWorkDimX] = {0}; /* k operations of multiply-add are divided into phases, each phase correspond to an * iteration of for-loop */ for (int ph = 0; ph \u0026lt; std::ceil((Value_t)k / kBlockTileDimK); ++ph) { LoadFromGmem\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kBlockTileDimK, strideA, strideB\u0026gt;( d_A, d_B, Ads_T, Bds, n, k, Ads_col, Ads_row, Bds_row, Bds_col); // Make sure all threads in block finished loading data __syncthreads(); // Advance tile d_A += kBlockTileDimK; // move kTileDimK columns to right d_B += kBlockTileDimK * n; // move kTileDimK rows down ProcessFromSmem\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kWarpWorkDimX, kWarpWorkDimY, kWarpTileWorkDimX, kWarpTileWorkDimY, kThreadWorkDimX, kThreadWorkDimY, kBlockTileDimK, kWarpTileNumY, kWarpTileNumX\u0026gt;( Ads_T, Bds, A_reg, B_reg, c_value, wy, wx, ty_in_acutal_warp, tx_in_acutal_warp); // Make sure all threads in block finished using shared memory, so that we can go // into next iteration __syncthreads(); } // Write results to GMEM for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) { // move C pointer to current warp subtile Value_t* d_C_interim = d_C + (w_sub_y * kWarpTileWorkDimY) * n + w_sub_x * kWarpTileWorkDimX; for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; cx += 4) reinterpret_cast\u0026lt;float4*\u0026gt;( \u0026amp;C_interim(ty_in_acutal_warp * kThreadWorkDimY + cy, tx_in_acutal_warp * kThreadWorkDimX + cx))[0] = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;c_value[w_sub_y][w_sub_x][cy][cx])[0]; } } 第八版实现：double buffer 这一版主要是引入了 double buffer，即让 GMEM -\u0026gt; SMEM 和 SMEM -\u0026gt; REG -\u0026gt; 计算 这两个部分可以流水线执行。先看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 #define A(i, j) d_A[(i) * k + (j)] #define B(i, j) d_B[(i) * n + (j)] #define C_interim(i, j) d_C_interim[(i) * n + (j)] template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kBlockTileDimK, int strideA, int strideB, typename T\u0026gt; __device__ void LoadFromGmem(float* d_A, float* d_B, float Ads_T[kBlockTileDimK][kBlockWorkDimY], float Bds[kBlockTileDimK][kBlockWorkDimX], int n, int k, int Ads_col, int Ads_row, int Bds_row, int Bds_col, T\u0026amp; barrier) { /* Collectively load data into shared memory */ for (int loadOffset = 0; loadOffset \u0026lt; kBlockWorkDimY; loadOffset += strideA) { //// Here A is transpoed and saved into SMEM cuda::memcpy_async(\u0026amp;Ads_T[Ads_col * 4 + 0][Ads_row + loadOffset], \u0026amp;A(Ads_row + loadOffset, Ads_col * 4 + 0), cuda::aligned_size_t\u0026lt;sizeof(float)\u0026gt;(sizeof(float)), barrier); cuda::memcpy_async(\u0026amp;Ads_T[Ads_col * 4 + 1][Ads_row + loadOffset], \u0026amp;A(Ads_row + loadOffset, Ads_col * 4 + 1), cuda::aligned_size_t\u0026lt;sizeof(float)\u0026gt;(sizeof(float)), barrier); cuda::memcpy_async(\u0026amp;Ads_T[Ads_col * 4 + 2][Ads_row + loadOffset], \u0026amp;A(Ads_row + loadOffset, Ads_col * 4 + 2), cuda::aligned_size_t\u0026lt;sizeof(float)\u0026gt;(sizeof(float)), barrier); cuda::memcpy_async(\u0026amp;Ads_T[Ads_col * 4 + 3][Ads_row + loadOffset], \u0026amp;A(Ads_row + loadOffset, Ads_col * 4 + 3), cuda::aligned_size_t\u0026lt;sizeof(float)\u0026gt;(sizeof(float)), barrier); } for (int loadOffset = 0; loadOffset \u0026lt; kBlockTileDimK; loadOffset += strideB) cuda::memcpy_async(\u0026amp;Bds[Bds_row + loadOffset][Bds_col * 4], \u0026amp;B(Bds_row + loadOffset, Bds_col * 4), cuda::aligned_size_t\u0026lt;sizeof(float4)\u0026gt;(sizeof(float4)), barrier); } template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kWarpWorkDimX, int kWarpWorkDimY, int kWarpTileWorkDimX, int kWarpTileWorkDimY, int kThreadWorkDimX, int kThreadWorkDimY, int kBlockTileDimK, int kWarpTileNumY, int kWarpTileNumX\u0026gt; __device__ void ProcessFromSmem( float Ads_T[kBlockTileDimK][kBlockWorkDimY], float Bds[kBlockTileDimK][kBlockWorkDimX], float A_reg[kWarpTileNumY][kThreadWorkDimY], float B_reg[kWarpTileNumX][kThreadWorkDimX], float c_value[kWarpTileNumY][kWarpTileNumX][kThreadWorkDimY][kThreadWorkDimX], int wy, int wx, int ty_in_acutal_warp, int tx_in_acutal_warp) { // Calculate per-thread results for (int k = 0; k \u0026lt; kBlockTileDimK; ++k) { // SMEM to registers // Due to transposed A in SMEM, the row and col indicies are switched for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int i = 0; i \u0026lt; kThreadWorkDimY; ++i) A_reg[w_sub_y][i] = Ads_T[k][wy * kWarpWorkDimY + w_sub_y * kWarpTileWorkDimY + ty_in_acutal_warp * kThreadWorkDimY + i]; for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) for (int i = 0; i \u0026lt; kThreadWorkDimX; ++i) B_reg[w_sub_x][i] = Bds[k][wx * kWarpWorkDimX + w_sub_x * kWarpTileWorkDimX + tx_in_acutal_warp * kThreadWorkDimX + i]; for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; ++cx) c_value[w_sub_y][w_sub_x][cy][cx] += A_reg[w_sub_y][cy] * B_reg[w_sub_x][cx]; } } // Tiled version with 2D thread coarsening, vectorized memory access and 2D warp tiling template \u0026lt;int kBlockWorkDimX, int kBlockWorkDimY, int kWarpWorkDimX, int kWarpWorkDimY, int kWarpTileWorkDimX, int kWarpTileWorkDimY, int kThreadWorkDimX, int kThreadWorkDimY, int kBlockTileDimK, int kThreadNumInBlock\u0026gt; __global__ void MyMatMulKernel(float* d_A, float* d_B, float* d_C, int m, int n, int k) { auto block = cooperative_groups::this_thread_block(); __shared__ cuda::barrier\u0026lt;cuda::thread_scope::thread_scope_block\u0026gt; frontBarrier; __shared__ cuda::barrier\u0026lt;cuda::thread_scope::thread_scope_block\u0026gt; backBarrier; auto frontBarrierPtr = \u0026amp;frontBarrier; auto backBarrierPtr = \u0026amp;backBarrier; if (block.thread_rank() == 0) { init(\u0026amp;frontBarrier, block.size()); init(\u0026amp;backBarrier, block.size()); } __syncthreads(); // Introduce kTileDimK, so the tile is not square, otherwise it would be too large // Here Ads is transposed __shared__ Value_t Ads_T[2][kBlockTileDimK][kBlockWorkDimY]; __shared__ Value_t Bds[2][kBlockTileDimK][kBlockWorkDimX]; const int by = blockIdx.y; const int bx = blockIdx.x; constexpr int kWarpSize = 32; const int warpIdx = threadIdx.x / kWarpSize; // the warp this thread is in const int wy = warpIdx / (kBlockWorkDimX / kWarpWorkDimX); const int wx = warpIdx % (kBlockWorkDimX / kWarpWorkDimX); // thread\u0026#39;s position w.r.t. warp tile (ACTUAL warp). const int threadIdxInWarp = threadIdx.x % kWarpSize; const int ty_in_acutal_warp = threadIdxInWarp / (kWarpTileWorkDimX / kThreadWorkDimX); const int tx_in_acutal_warp = threadIdxInWarp % (kWarpTileWorkDimX / kThreadWorkDimX); // Move tile to beginning of d_A\u0026#39;s row and d_B\u0026#39;s column d_A += by * kBlockWorkDimY * k; d_B += bx * kBlockWorkDimX; // Here output position needs to go to warp output position d_C += (by * kBlockWorkDimY + wy * kWarpWorkDimY) * n + bx * kBlockWorkDimX + wx * kWarpWorkDimX; // Calculate the indices that this thread will load into SMEM const int Ads_row = threadIdx.x / (kBlockTileDimK / 4); const int Ads_col = threadIdx.x % (kBlockTileDimK / 4); const int Bds_row = threadIdx.x / (kBlockWorkDimX / 4); const int Bds_col = threadIdx.x % (kBlockWorkDimX / 4); constexpr int strideA = kThreadNumInBlock / (kBlockTileDimK / 4); constexpr int strideB = kThreadNumInBlock / (kBlockWorkDimX / 4); constexpr int kWarpTileNumY = kWarpWorkDimY / kWarpTileWorkDimY; constexpr int kWarpTileNumX = kWarpWorkDimX / kWarpTileWorkDimX; // Register caches for Ads and Bds (on the warptile level) Value_t A_reg[kWarpTileNumY][kThreadWorkDimY] = {0.0}; Value_t B_reg[kWarpTileNumX][kThreadWorkDimX] = {0.0}; // Results are in register file in warp scheduler Value_t c_value[kWarpTileNumY][kWarpTileNumX][kThreadWorkDimY][kThreadWorkDimX] = {0}; int curr_buffer_idx = 0; // Double-buffering: load first blocktile into SMEM LoadFromGmem\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kBlockTileDimK, strideA, strideB\u0026gt;( d_A, d_B, Ads_T[curr_buffer_idx], Bds[curr_buffer_idx], n, k, Ads_col, Ads_row, Bds_row, Bds_col, *frontBarrierPtr); // Advance tile after loading GMEM d_A += kBlockTileDimK; // move kTileDimK columns to right d_B += kBlockTileDimK * n; // move kTileDimK rows down /* k operations of multiply-add are divided into phases, each phase correspond to an * iteration of for-loop */ for (int ph = 0; ph \u0026lt; std::ceil((Value_t)k / kBlockTileDimK) - 1; ++ph) { LoadFromGmem\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kBlockTileDimK, strideA, strideB\u0026gt;( d_A, d_B, Ads_T[1 - curr_buffer_idx], Bds[1 - curr_buffer_idx], n, k, Ads_col, Ads_row, Bds_row, Bds_col, *backBarrierPtr); // Advance tile after loading GMEM d_A += kBlockTileDimK; // move kTileDimK columns to right d_B += kBlockTileDimK * n; // move kTileDimK rows down // Make sure all threads in block finished loading data frontBarrierPtr-\u0026gt;arrive_and_wait(); ProcessFromSmem\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kWarpWorkDimX, kWarpWorkDimY, kWarpTileWorkDimX, kWarpTileWorkDimY, kThreadWorkDimX, kThreadWorkDimY, kBlockTileDimK, kWarpTileNumY, kWarpTileNumX\u0026gt;( Ads_T[curr_buffer_idx], Bds[curr_buffer_idx], A_reg, B_reg, c_value, wy, wx, ty_in_acutal_warp, tx_in_acutal_warp); // Make sure all threads in block finished loading data backBarrierPtr-\u0026gt;arrive_and_wait(); // Exchange buffer indices and barriers curr_buffer_idx = 1 - curr_buffer_idx; std::swap(frontBarrierPtr, backBarrierPtr); } // Compute the last blocktile frontBarrierPtr-\u0026gt;arrive_and_wait(); ProcessFromSmem\u0026lt;kBlockWorkDimX, kBlockWorkDimY, kWarpWorkDimX, kWarpWorkDimY, kWarpTileWorkDimX, kWarpTileWorkDimY, kThreadWorkDimX, kThreadWorkDimY, kBlockTileDimK, kWarpTileNumY, kWarpTileNumX\u0026gt;( Ads_T[curr_buffer_idx], Bds[curr_buffer_idx], A_reg, B_reg, c_value, wy, wx, ty_in_acutal_warp, tx_in_acutal_warp); // Write results to GMEM for (int w_sub_y = 0; w_sub_y \u0026lt; kWarpTileNumY; ++w_sub_y) for (int w_sub_x = 0; w_sub_x \u0026lt; kWarpTileNumX; ++w_sub_x) { // move C pointer to current warp subtile Value_t* d_C_interim = d_C + (w_sub_y * kWarpTileWorkDimY) * n + w_sub_x * kWarpTileWorkDimX; for (int cy = 0; cy \u0026lt; kThreadWorkDimY; ++cy) for (int cx = 0; cx \u0026lt; kThreadWorkDimX; cx += 4) reinterpret_cast\u0026lt;float4*\u0026gt;( \u0026amp;C_interim(ty_in_acutal_warp * kThreadWorkDimY + cy, tx_in_acutal_warp * kThreadWorkDimX + cx))[0] = reinterpret_cast\u0026lt;float4*\u0026gt;(\u0026amp;c_value[w_sub_y][w_sub_x][cy][cx])[0]; } } 我们可以看到，最大的区别是，从 GMEM -\u0026gt; SMEM 这一部分是换成了 memcpy_async()，这样不会堵塞后面的执行，并且依靠 cuda::barrier 来执行同步。可以看到两块 shared memory 分别加载两块 GMEM，并依赖于不同的 barrier，这样可以让不同块的 shared memory 分别进行“读 GMEM”和“到 REG 计算”这两个过程。\n","date":"2025-02-22T21:32:50+08:00","permalink":"https://georgelyu.github.io/p/cuda_gemm_opt/","title":"CUDA GEMM 优化"}]