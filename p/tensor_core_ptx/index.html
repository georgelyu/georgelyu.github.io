<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="本篇文章主要讲如何通过 mma ptx 语句来调用 NVIDIA GPU 的 Tensor Core。\n主要概念 Tensor Core 执行的是 M-by-N-by-K 的矩阵操作 D = op(A, B) + C，这里使用的是 BLAS 的定义，即 A 矩阵的维度是 M x K，B 矩阵的维度是 K x N，C 和 D 矩阵的维度是 M x N。我们把 [M, N, K] 的组合称为 shape。\n"><title>通过 MMA 使用 NVIDIA GPU 的 Tensor Core</title>
<link rel=canonical href=https://georgelyu.github.io/p/tensor_core_ptx/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="通过 MMA 使用 NVIDIA GPU 的 Tensor Core"><meta property='og:description' content="本篇文章主要讲如何通过 mma ptx 语句来调用 NVIDIA GPU 的 Tensor Core。\n主要概念 Tensor Core 执行的是 M-by-N-by-K 的矩阵操作 D = op(A, B) + C，这里使用的是 BLAS 的定义，即 A 矩阵的维度是 M x K，B 矩阵的维度是 K x N，C 和 D 矩阵的维度是 M x N。我们把 [M, N, K] 的组合称为 shape。\n"><meta property='og:url' content='https://georgelyu.github.io/p/tensor_core_ptx/'><meta property='og:site_name' content='吕超阳的博客'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2025-03-25T14:23:56+08:00'><meta property='article:modified_time' content='2025-03-25T14:23:56+08:00'><meta name=twitter:title content="通过 MMA 使用 NVIDIA GPU 的 Tensor Core"><meta name=twitter:description content="本篇文章主要讲如何通过 mma ptx 语句来调用 NVIDIA GPU 的 Tensor Core。\n主要概念 Tensor Core 执行的是 M-by-N-by-K 的矩阵操作 D = op(A, B) + C，这里使用的是 BLAS 的定义，即 A 矩阵的维度是 M x K，B 矩阵的维度是 K x N，C 和 D 矩阵的维度是 M x N。我们把 [M, N, K] 的组合称为 shape。\n"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_c68a00bbf16dac8.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>吕超阳的博客</a></h1><h2 class=site-description>写点想写的东西。</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>存档</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#主要概念>主要概念</a><ol><li><a href=#fundamental-shape>Fundamental Shape</a></li><li><a href=#扩展-m-和-k-维度>扩展 M 和 K 维度</a></li></ol></li><li><a href=#数据的加载>数据的加载</a><ol><li><a href=#低延迟地加载-global-memory>低延迟地加载 global memory</a></li><li><a href=#无冲突的-shared-memory-存储--读取>无冲突的 shared memory 存储 / 读取</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/cuda/>CUDA</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/tensor_core_ptx/>通过 MMA 使用 NVIDIA GPU 的 Tensor Core</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2025 年 3 月 25 日</time></div></footer></div></header><section class=article-content><p>本篇文章主要讲如何通过 <code>mma</code> ptx 语句来调用 NVIDIA GPU 的 Tensor Core。</p><h2 id=主要概念>主要概念</h2><p>Tensor Core 执行的是 M-by-N-by-K 的矩阵操作 <code>D = op(A, B) + C</code>，这里使用的是 BLAS 的定义，即 <code>A</code> 矩阵的维度是 <code>M x K</code>，<code>B</code> 矩阵的维度是 <code>K x N</code>，<code>C</code> 和 <code>D</code> 矩阵的维度是 <code>M x N</code>。我们把 <code>[M, N, K]</code> 的组合称为 shape。</p><p>Tensor Core 在执行这样的矩阵操作时是 warp-synchronous 的模式，即 warp 中所有线程同时参与运算。而对于拥有 32 个线程的 warp 来说，很明显有一些 shape 对于 warp 来说会比较好处理，如下面讲的 fundamental shape。</p><h3 id=fundamental-shape>Fundamental Shape</h3><p>Tensor Core 操作中的 fundamental shape 是 8-by-8-by-128b，见下图。</p><p><img src=/p/tensor_core_ptx/p_15.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_15_hu_6098c11da8d2f790.jpg 480w, /p/tensor_core_ptx/p_15_hu_2ce569908f52ec8c.jpg 1024w" loading=lazy alt="一个 8-by-8-by-128b 的 warp-wide Tensor Core 操作。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>在图中，蓝色对应矩阵 A，黄色对应矩阵 B，绿色则对应矩阵 C 或 D。因为一个 warp 有 32 个线程，我们可以把这 32 个线程排布成一个 8 x 4 和 4 x 8 的矩阵，分别对应蓝色和黄色部分。如果一个线程持有一个 32b 的数据（可以以图中的 T0 为视角），则依据图示，我们可以看到 K 维度是 128b，所以我们称这个操作是 8-by-8-by-128b 的。我们注意到 T0 拥有两个 accumulator，为了获得 T0 线程的结果，我们需要的是矩阵 A（图中蓝色）的第一行（即 T0-T3 部分），和矩阵 B（图中黄色）的第一、二列（即 T0-T7 部分）。</p><p>这个示例就对应了一个实际的 Tensor Core 操作，即 <a class=link href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-8816 target=_blank rel=noopener><code>mma.m8n8k16</code></a>，见下图。</p><p><img src=/p/tensor_core_ptx/p_16.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_16_hu_367cd981c9417438.jpg 480w, /p/tensor_core_ptx/p_16_hu_3a59b06095103840.jpg 1024w" loading=lazy alt="mma.m8n8k16 的图解。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>这里 <code>mma.m8n8k16</code> 针对的场景是 S8 * S8 + S32，即 A 和 B 的类型是有符号单字节整型，accumulator 是有符号 32b 整型。所以我们看到图中，在矩阵 A 中，T0 拥有的 32 位数据对应了 4 个元素，以 little-endian 形式存储。图右侧显示了实际的 ptx 指令，这里每个线程只需要开一个 32b 寄存器作为输入（A 和 B），但是需要两个 32b 寄存器作为 accumulator，因为每个结果元素是以 32b 进行存储的。</p><p>这里顺便讲一下这里 <code>mma</code> 指令的语法，大体上的格式是</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-.asm data-lang=.asm><span class=line><span class=cl><span class=nf>mma.sync.aligned.shape.alayout.blayout.dtype.atype.btype.ctype</span>  <span class=nv>d</span><span class=p>,</span> <span class=nv>a</span><span class=p>,</span> <span class=nv>b</span><span class=p>,</span> <span class=nv>c</span><span class=c1>;</span>
</span></span></code></pre></td></tr></table></div></div><p>这里</p><ul><li>mma 是表示 Matrix Multiply-and-Accumulate</li><li>sync 表示这句指令之前隐含一个 warp-level sync</li><li>aligned 表示 warp 内所有线程都会执行这同一句指令，如果这一点没有被保证会得到 undefined behavior</li><li>shape 就是这里讲的 [M, N, K] 组合</li><li>alayout、blayout 指的是矩阵 A 和 B 的数据排布（row-major 或 column-major）</li><li>dtype、atype、btype、ctype 指的就是这些矩阵的数据类型了</li></ul><p>当然这些参数之间互相还有一些限制，还有一些额外的参数，详情参考<a class=link href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma target=_blank rel=noopener>官方文档</a>。</p><h3 id=扩展-m-和-k-维度>扩展 M 和 K 维度</h3><p>在有了 fundamental shape 之后，如果我们想要算更大的 shape 怎么办。很简单的一个想法是，我们直接按照上面的方式再算一次，先从 M 维度开始（见下图）。</p><p><img src=/p/tensor_core_ptx/p_17.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_17_hu_d16d10900b8f8af4.jpg 480w, /p/tensor_core_ptx/p_17_hu_30547ac40c5b5c0f.jpg 1024w" loading=lazy alt="对 fundamental shape 在 M 维度进行扩展，即 16-by-8-by-128b 操作。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>这对应了 F16 * F16 + F32 下面的 <code>mma.m16n8k8</code> 操作。</p><p><img src=/p/tensor_core_ptx/p_18.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_18_hu_38b3192e9a44a739.jpg 480w, /p/tensor_core_ptx/p_18_hu_72be15b01deb8b70.jpg 1024w" loading=lazy alt="mma.m16n8k8 的图解。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>那么当然，同理，K 维度也可以得到提升，得到 16-by-8-by-256b 操作。</p><p><img src=/p/tensor_core_ptx/p_19.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_19_hu_5c63b41d29a064b3.jpg 480w, /p/tensor_core_ptx/p_19_hu_e82b7bbf463ba7a0.jpg 1024w" loading=lazy alt="对 fundamental shape 在 M 和 K 维度进行扩展，即 16-by-8-by-256b 操作。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>这对应了 F16 * F16 + F32 下面的 <code>mma.m16n8k16</code> 操作。</p><p><img src=/p/tensor_core_ptx/p_20.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_20_hu_daa0064c02cf75a2.jpg 480w, /p/tensor_core_ptx/p_20_hu_5bc5253b982795d2.jpg 1024w" loading=lazy alt="mma.m16n8k16 的图解。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>同时也对应了 S8 * S8 + F32 下面的 <code>mma.m16n8k32</code> 操作。</p><p><img src=/p/tensor_core_ptx/p_21.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_21_hu_2a24e8d80b58a00e.jpg 480w, /p/tensor_core_ptx/p_21_hu_18f680bc64dd665b.jpg 1024w" loading=lazy alt="mma.m16n8k32 的图解。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><h2 id=数据的加载>数据的加载</h2><p>我们看到，<code>mma</code> 语句主要负责矩阵的计算部分，而矩阵数据的加载也是重要的一环。我们先展示一个 <code>mma</code> 语句完整的 Hellow World 示例（见下图）。</p><p><img src=/p/tensor_core_ptx/p_27.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_27_hu_e3bcdec9681efc7a.jpg 480w, /p/tensor_core_ptx/p_27_hu_faf25159a60b4daf.jpg 1024w" loading=lazy alt="mma 语句的 Hellow World 示例。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>而我们分析一下上图中的 kernel 就可以发现这个 kernel 是严重的 bandwidth-bound（分析过程见下图）。</p><p><img src=/p/tensor_core_ptx/p_28.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_28_hu_8ba1c2196b09091b.jpg 480w, /p/tensor_core_ptx/p_28_hu_e3a01a5b715bbafe.jpg 1024w" loading=lazy alt="直接加载数据会造成程序 bandwidth-bound。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>为了加快数据的加载，我们主要有三点可以做：</p><ul><li>低延迟地加载 global memory</li><li>无冲突的 shared memory 存储</li><li>无冲突的 shared memory 读取</li></ul><p>下面我们每一点分别来讲。</p><h3 id=低延迟地加载-global-memory>低延迟地加载 global memory</h3><p>这里主要的做法是使用 Async Copy（<code>cp.async</code>）。</p><p>这个做法有三点好处：</p><ul><li>数据的移动路径变短：之前数据是从 L2 → L1 → RF → SMEM，而 <code>cp.async</code> 会让数据直接从 L2 → SMEM；</li><li>减少寄存器用量：因为 RF 被绕过了，所以寄存器的用量也减少了。而且因为这些 load 语句的延迟很高，这些寄存器其实被长时间占用，不能有效地分给 kernel 的其他部分；</li><li>间接影响：异步拷贝现在可以让我们在 kernel 中进行多 stage 操作，这样我们可以在 kernel 计算时去取数据。如下图最下面的部分，我们可以看到展示了一段循环内存，其中 committed stage 是正在从 SMEM 取数据进行计算的部分，而后面有三个阶段在等待取数据，这样最大限度地喂饱计算单元。</li></ul><p><img src=/p/tensor_core_ptx/p_31.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_31_hu_3574a9aa355101b2.jpg 480w, /p/tensor_core_ptx/p_31_hu_6c7b93f19966326e.jpg 1024w" loading=lazy alt="Async Copy 的好处。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><h3 id=无冲突的-shared-memory-存储--读取>无冲突的 shared memory 存储 / 读取</h3><p>这里我们先介绍一下 <code>ldmatrix</code> 指令。<code>ldmatrix</code> 是一类特殊的，为了加载 tensor core 所需数据而出现的指令。它加载进来的数据排布是完美迎合了 Tensor Core 操作的。我们看到下图的右下角部分。</p><p><img src=/p/tensor_core_ptx/p_34.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_34_hu_d4cf383c71227f3e.jpg 480w, /p/tensor_core_ptx/p_34_hu_3bba754c28be5029.jpg 1024w" loading=lazy alt="ldmatrix 指令的图示。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p><code>ldmatrix</code> 工作的原理是，每个线程会有一个指针，指向一行 128b 的数据（见蓝色中黑框框中的 T0-T3 部分），这个线程会读取这 128b 的数据，然后广播给真正需要它的线程们。下图是实际 ptx 指令的示例，我们可以看到这里用了 x4 的指令，即加载了四个蓝色部分的矩阵，所以每个线程加载的数据都没有浪费。</p><p><img src=/p/tensor_core_ptx/p_35.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_35_hu_e730ae6c8d1d6a1b.jpg 480w, /p/tensor_core_ptx/p_35_hu_b5de95f38dda1bc.jpg 1024w" loading=lazy alt="ldmatrix 指令的实际 ptx 使用示例。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>然后为了避免 bank-conflict，我们需要使用 permuted indices，如下图。</p><p><img src=/p/tensor_core_ptx/p_39.jpg width=3000 height=1688 srcset="/p/tensor_core_ptx/p_39_hu_7d70b1627388eb47.jpg 480w, /p/tensor_core_ptx/p_39_hu_d0f25fe34e1c0de0.jpg 1024w" loading=lazy alt="使用 permuted indices 来避免 bank conlifct。" class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>这个操作需要我们在使用 <code>ldmatrix</code> 进行加载时，把每个线程得到的序号手动错开。</p></section><footer class=article-footer></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/ptx/><div class=article-details><h2 class=article-title>在 CUDA 中使用 PTX</h2></div></a></article><article><a href=/p/cuda_softmax_opt/><div class=article-details><h2 class=article-title>CUDA Softmax 优化</h2></div></a></article><article><a href=/p/cuda_bank_conflict/><div class=article-details><h2 class=article-title>CUDA Bank Conflict 的解决方法</h2></div></a></article><article><a href=/p/cuda_cooperative_groups/><div class=article-details><h2 class=article-title>《Cooperative Groups Flexible CUDA Thread Programming》笔记</h2></div></a></article><article><a href=/p/lower_occupancy_note/><div class=article-details><h2 class=article-title>《Better Performance at Lower Occupancy》笔记</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2025 吕超阳的博客</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>